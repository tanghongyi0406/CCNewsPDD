{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNa7/sFvYNTmsD/z5YzzZyc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanghongyi0406/CCNewsPDD/blob/main/Code/NAPDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pip and Import\n"
      ],
      "metadata": {
        "id": "pAno9jmkprMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BVwv034oscH"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip uninstall -y torch torchvision torchaudio transformers datasets accelerate bitsandbytes\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers==4.37.2\n",
        "!pip install peft==0.10.0\n",
        "!pip install bitsandbytes==0.42.0\n",
        "!pip install accelerate==0.27.0\n",
        "!pip install datasets==2.19.0\n",
        "!pip install matplotlib==3.9.0 \\\n",
        "            numpy==1.26.4 \\\n",
        "            pandas==2.2.2 \\\n",
        "            scikit_learn==1.4.2 \\\n",
        "            tqdm==4.66.2 \\\n",
        "            deepspeed==0.14.0\n",
        "\n",
        "# restart Runtime1\n",
        "import IPython\n",
        "IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "QAKduDMoo1fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "from simple_parsing.helpers import Serializable, field\n",
        "\n",
        "# Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the root folder to LLM_MIA.\n",
        "base_dir = \"/content/drive/MyDrive/LLM_MIA\"\n",
        "\n",
        "# Set the paths for each subfolder.\n",
        "cache_dir = os.path.join(base_dir, \"cache_dir\")\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "results_dir = os.path.join(base_dir, \"results\")\n",
        "\n",
        "# Set MIMIR environment variables\n",
        "os.environ['MIMIR_CACHE_PATH'] = cache_dir\n",
        "os.environ['MIMIR_DATA_SOURCE'] = data_dir\n",
        "\n",
        "# Create the folder structure.\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(f\"All data is stored in the data directory.: {base_dir}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VWv-aMP4qatM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "NK8YQ1JA3FpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back Translate"
      ],
      "metadata": {
        "id": "lDeJuK7A3HcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import zstandard as zstd\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "\n",
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# ============== Part 1: Google Drive Mount & Utility Functions ==============\n",
        "\n",
        "# Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def read_zst_jsonl(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            dctx = zstd.ZstdDecompressor()\n",
        "            with dctx.stream_reader(f) as reader:\n",
        "                with io.TextIOWrapper(reader, encoding=\"utf-8\") as text_reader:\n",
        "                    for line in tqdm(text_reader, desc=f\"load {os.path.basename(file_path)}\"):\n",
        "                        try:\n",
        "                            yield line.strip()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error processing line: {e}\")\n",
        "                            continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_jsonl_to_dataset(file_path, filter_func=None, limit=None):\n",
        "    try:\n",
        "        data = []\n",
        "        count = 0\n",
        "        total_processed = 0\n",
        "        rejected = 0\n",
        "\n",
        "        for line in read_zst_jsonl(file_path):\n",
        "            total_processed += 1\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                if filter_func is None or filter_func(item):\n",
        "                    data.append(item)\n",
        "                    count += 1\n",
        "                    if limit and count >= limit:\n",
        "                        break\n",
        "                else:\n",
        "                    rejected += 1\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON line: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Processed {total_processed} lines; after filtering, selected {count} samples and rejected {rejected} samples.\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "def is_pile_cc_data(item):\n",
        "    try:\n",
        "        content = item.get('text', '')\n",
        "        if len(content) < 512:\n",
        "            return item.get('meta', {}).get('pile_set_name') == \"Pile-CC\"\n",
        "        else:\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def stream_pile_cc_data(limit=None):\n",
        "    count = 0\n",
        "    pile_cc_data = []\n",
        "    total_processed = 0\n",
        "    rejected_long = 0\n",
        "\n",
        "    ds_stream = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\", streaming=True)\n",
        "\n",
        "    for item in tqdm(ds_stream, desc=\"load Pile-CC data\"):\n",
        "        total_processed += 1\n",
        "        content = item.get('text', '')\n",
        "\n",
        "        if item['meta']['pile_set_name'] == \"Pile-CC\" and len(content) < 512:\n",
        "            pile_cc_data.append(item)\n",
        "            count += 1\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "        elif item['meta']['pile_set_name'] == \"Pile-CC\":\n",
        "            rejected_long += 1\n",
        "\n",
        "    print(f\"Total processed: {total_processed}; selected {len(pile_cc_data)} eligible Pile-CC entries.\")\n",
        "    print(f\"Pile-CC entries rejected due to length > 512 characters: {rejected_long}\")\n",
        "    return pile_cc_data\n",
        "\n",
        "def check_content_overlap(ds_train, ds_dev, ds_test):\n",
        "    train_texts = set(ds_train['text'])\n",
        "    dev_texts = set(ds_dev['text'])\n",
        "    test_texts = set(ds_test['text'])\n",
        "\n",
        "    train_dev_overlap = train_texts.intersection(dev_texts)\n",
        "    train_test_overlap = train_texts.intersection(test_texts)\n",
        "    dev_test_overlap = dev_texts.intersection(test_texts)\n",
        "\n",
        "    return len(train_dev_overlap), len(train_test_overlap), len(dev_test_overlap)\n",
        "\n",
        "def check_sample_lengths(dataset, name):\n",
        "    lengths = [len(text) for text in dataset['text']]\n",
        "    max_length = max(lengths)\n",
        "    min_length = min(lengths)\n",
        "    avg_length = sum(lengths) / len(lengths)\n",
        "\n",
        "    over_512 = sum(1 for l in lengths if l >= 512)\n",
        "\n",
        "    print(f\"\\n{name} length check:\")\n",
        "    print(f\"Number of samples: {len(dataset)}\")\n",
        "    print(f\"Min length: {min_length}, Max length: {max_length}, Avg length: {avg_length:.1f}\")\n",
        "    print(f\"Samples ≥ 512 characters: {over_512}/{len(dataset)} ({over_512/len(dataset)*100:.1f}%)\")\n",
        "\n",
        "    lengths_0 = [len(dataset['text'][i]) for i in range(len(dataset)) if dataset['label'][i] == 0]\n",
        "    lengths_1 = [len(dataset['text'][i]) for i in range(len(dataset)) if dataset['label'][i] == 1]\n",
        "\n",
        "    print(f\"label 0 sample: number={len(lengths_0)}, avg length={sum(lengths_0)/max(1,len(lengths_0)):.1f}\")\n",
        "    print(f\"label 1 sample: number={len(lengths_1)}, avg length={sum(lengths_1)/max(1,len(lengths_1)):.1f}\")\n",
        "\n",
        "    return lengths\n",
        "\n",
        "def filter_text_by_length(text, max_length=512):\n",
        "    return len(text) < max_length\n",
        "\n",
        "# ============== Part 2: Back-Translation (EN → FR → EN) — Required Functions & Model Initialization ==============\n",
        "def load_translation_model(model_name):\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=None,\n",
        "        low_cpu_mem_usage=False\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "# Translation Model\n",
        "en2fr_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "fr2en_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
        "\n",
        "tokenizer_en2fr, model_en2fr = load_translation_model(en2fr_name)\n",
        "\n",
        "tokenizer_fr2en, model_fr2en = load_translation_model(fr2en_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_en2fr.to(device)\n",
        "model_fr2en.to(device)\n",
        "\n",
        "def translate(texts, tokenizer, model, max_length=512):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(**inputs, max_length=max_length)\n",
        "    outputs = [tokenizer.decode(t, skip_special_tokens=True) for t in generated]\n",
        "    return outputs\n",
        "\n",
        "def back_translate_batch(batch):\n",
        "    input_texts = batch[\"text\"]\n",
        "    # 1) E->F\n",
        "    fr_texts = translate(input_texts, tokenizer_en2fr, model_en2fr)\n",
        "    # 2) F->E\n",
        "    en_texts = translate(fr_texts, tokenizer_fr2en, model_fr2en)\n",
        "    return {\"text\": en_texts}\n",
        "\n",
        "# ============== Part 3: Main Pipeline ==============\n",
        "def main():\n",
        "    data_dir = \"/content/drive/MyDrive/LLM_MIA/data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # 1) load CC News data\n",
        "        cc_news = load_dataset(\"vblagoje/cc_news\")\n",
        "        print(f\"Loaded successfully. CC-News dataset size: {len(cc_news['train'])}\")\n",
        "\n",
        "        # 2) Filter CC-News texts with length < 512 characters\n",
        "        filtered_cc_news_texts = []\n",
        "        for item in tqdm(cc_news['train'], desc=\"Filtering CC-News dataset\"):\n",
        "            if len(item['text']) < 512:\n",
        "                filtered_cc_news_texts.append(item['text'])\n",
        "\n",
        "        print(f\"Filtered CC-News dataset size: {len(filtered_cc_news_texts)}\")\n",
        "        if len(filtered_cc_news_texts) < 1000:\n",
        "            raise ValueError(f\"CC-News data is insufficient: only {len(filtered_cc_news_texts)} samples; at least 1000 are required.\")\n",
        "\n",
        "        # 3) load Pile-CC data\n",
        "        pile_cc_data = stream_pile_cc_data(limit=1000)\n",
        "        pile_cc_texts = [item['text'] for item in pile_cc_data]\n",
        "        print(f\"Number of Pile-CC: {len(pile_cc_texts)}\")\n",
        "\n",
        "        random.seed(42)\n",
        "\n",
        "        # 4) split data\n",
        "        random.shuffle(filtered_cc_news_texts)\n",
        "        train_size = 200\n",
        "        dev_nonmember_size = 200\n",
        "        test_nonmember_size = 400\n",
        "\n",
        "        required_cc_news = train_size + dev_nonmember_size + test_nonmember_size\n",
        "        if len(filtered_cc_news_texts) < required_cc_news:\n",
        "            raise ValueError(f\"Insufficient CC-News data: need {required_cc_news} samples, only {len(filtered_cc_news_texts)} available.\")\n",
        "\n",
        "        train_cc_news = filtered_cc_news_texts[:train_size]\n",
        "        dev_nonmember = filtered_cc_news_texts[train_size:train_size+dev_nonmember_size]\n",
        "        test_nonmember = filtered_cc_news_texts[train_size+dev_nonmember_size:train_size+dev_nonmember_size+test_nonmember_size]\n",
        "\n",
        "        # Pile-CC split\n",
        "        dev_member_size = 200\n",
        "        test_member_size = 400\n",
        "        required_pile_cc = dev_member_size + test_member_size\n",
        "        if len(pile_cc_texts) < required_pile_cc:\n",
        "            raise ValueError(f\"Insufficient Pile-CC data: need{required_pile_cc}samples, only{len(pile_cc_texts)}available.\")\n",
        "\n",
        "        random.shuffle(pile_cc_texts)\n",
        "        dev_member = pile_cc_texts[:dev_member_size]\n",
        "        test_member = pile_cc_texts[dev_member_size:dev_member_size+test_member_size]\n",
        "\n",
        "        # build train: 200 samples -> 100(member)+100(nonmember)\n",
        "        random.shuffle(train_cc_news)\n",
        "        train_member = train_cc_news[:train_size//2]\n",
        "        train_nonmember = train_cc_news[train_size//2:train_size]\n",
        "\n",
        "        ds_train = Dataset.from_dict({\n",
        "            'text': train_member + train_nonmember,\n",
        "            'label': [1]* (train_size//2) + [0]*(train_size//2)\n",
        "        })\n",
        "\n",
        "        # bulid dev: 200(member)+200(nonmember)\n",
        "        ds_dev = Dataset.from_dict({\n",
        "            'text': dev_member + dev_nonmember,\n",
        "            'label': [1]*dev_member_size + [0]*dev_nonmember_size\n",
        "        })\n",
        "\n",
        "        # bulid test: 400(member)+400(nonmember)\n",
        "        ds_test = Dataset.from_dict({\n",
        "            'text': test_member + test_nonmember,\n",
        "            'label': [1]*test_member_size + [0]*test_nonmember_size\n",
        "        })\n",
        "\n",
        "        # ========== Check dataset overlap & print info ==========\n",
        "        train_dev_overlap, train_test_overlap, dev_test_overlap = check_content_overlap(ds_train, ds_dev, ds_test)\n",
        "        print(\"\\nDataset overlap check:\")\n",
        "        print(f\"Train-Dev overlap: {train_dev_overlap}\")\n",
        "        print(f\"Train-Test overlap: {train_test_overlap}\")\n",
        "        print(f\"Dev-Test overlap: {dev_test_overlap}\")\n",
        "        if train_dev_overlap > 0 or train_test_overlap > 0 or dev_test_overlap > 0:\n",
        "            print(\"warning：Overlap detected between datasets!\")\n",
        "\n",
        "        check_sample_lengths(ds_train, \"train\")\n",
        "        check_sample_lengths(ds_dev, \"dev\")\n",
        "        check_sample_lengths(ds_test, \"test\")\n",
        "\n",
        "        print(\"\\n data size:\")\n",
        "        print(f\"Train size: {len(ds_train)}\")\n",
        "        print(f\"Dev size:   {len(ds_dev)}\")\n",
        "        print(f\"Test size:  {len(ds_test)}\")\n",
        "\n",
        "        # ============== For comparability, keep `original_text` for now. ==============\n",
        "        ds_train = ds_train.add_column(\"original_text\", ds_train[\"text\"])\n",
        "        ds_dev = ds_dev.add_column(\"original_text\", ds_dev[\"text\"])\n",
        "        ds_test = ds_test.add_column(\"original_text\", ds_test[\"text\"])\n",
        "\n",
        "        # 5) Back-Translation\n",
        "        # need：Train all back-translation；Dev/Test only back-translation nonmember (label=0)\n",
        "\n",
        "        print(\"\\n1) Back-translate training set (all samples)...\")\n",
        "        ds_train = ds_train.map(\n",
        "            back_translate_batch,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            desc=\"Back-translate train (member + nonmember)\"\n",
        "        )\n",
        "\n",
        "        print(\"\\n2) Back-translate validation set nonmembers only...\")\n",
        "        # Split dev into dev_member / dev_nonmember\n",
        "        ds_dev_member = ds_dev.filter(lambda x: x[\"label\"] == 1)\n",
        "        ds_dev_nonmember = ds_dev.filter(lambda x: x[\"label\"] == 0)\n",
        "\n",
        "        # Back-translate nonmembers\n",
        "        ds_dev_nonmember = ds_dev_nonmember.map(\n",
        "            back_translate_batch,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            desc=\"Back-translate dev nonmembers\"\n",
        "        )\n",
        "\n",
        "        # Merge back into dev and shuffle\n",
        "        ds_dev = concatenate_datasets([ds_dev_member, ds_dev_nonmember]).shuffle(seed=42)\n",
        "\n",
        "        print(\"\\n3) Back-translate test set nonmembers only...\")\n",
        "        ds_test_member = ds_test.filter(lambda x: x[\"label\"] == 1)\n",
        "        ds_test_nonmember = ds_test.filter(lambda x: x[\"label\"] == 0)\n",
        "\n",
        "        ds_test_nonmember = ds_test_nonmember.map(\n",
        "            back_translate_batch,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            desc=\"Back-translate test nonmembers\"\n",
        "        )\n",
        "\n",
        "        ds_test = concatenate_datasets([ds_test_member, ds_test_nonmember]).shuffle(seed=42)\n",
        "\n",
        "        # 6) Save data\n",
        "        ds_train.save_to_disk(os.path.join(data_dir, \"pile_cc_mia_train_bt\"))\n",
        "        ds_dev.save_to_disk(os.path.join(data_dir, \"pile_cc_mia_dev_bt\"))\n",
        "        ds_test.save_to_disk(os.path.join(data_dir, \"pile_cc_mia_test_bt\"))\n",
        "\n",
        "        print(\"\\nBack-translated datasets have been successfully saved to:\")\n",
        "        print(\" -\", os.path.join(data_dir, \"pile_cc_mia_train_bt\"))\n",
        "        print(\" -\", os.path.join(data_dir, \"pile_cc_mia_dev_bt\"))\n",
        "        print(\" -\", os.path.join(data_dir, \"pile_cc_mia_test_bt\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"error: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gWc5HPctsLRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "data_dir = \"/content/drive/MyDrive/LLM_MIA/data\"\n",
        "\n",
        "ds_train = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_train_bt\"))\n",
        "ds_dev = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_dev_bt\"))\n",
        "ds_test = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_test_bt\"))\n",
        "\n",
        "print(f\"train size: {len(ds_train)}\")\n",
        "print(f\"dev size: {len(ds_dev)}\")\n",
        "print(f\"test size: {len(ds_test)}\")\n",
        "\n",
        "print(ds_test[0])"
      ],
      "metadata": {
        "id": "pXbKJP6Lo3Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert Mask"
      ],
      "metadata": {
        "id": "PlqGL4v63MzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import zstandard as zstd\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "###############################################################################\n",
        "# Part 1: Google Drive Mount & Utility Functions\n",
        "###############################################################################\n",
        "\n",
        "# 1) Mount Google Drive (can be skipped if already mounted in Colab)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def read_zst_jsonl(file_path):\n",
        "    \"\"\"\n",
        "    Reads a .zst compressed JSONL file, yielding one string per line.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            dctx = zstd.ZstdDecompressor()\n",
        "            with dctx.stream_reader(f) as reader:\n",
        "                with io.TextIOWrapper(reader, encoding=\"utf-8\") as text_reader:\n",
        "                    for line in tqdm(text_reader, desc=f\"Loading {os.path.basename(file_path)}\"):\n",
        "                        try:\n",
        "                            yield line.strip()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error processing line: {e}\")\n",
        "                            continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_jsonl_to_dataset(file_path, filter_func=None, limit=None):\n",
        "    \"\"\"\n",
        "    Loads a zst-jsonl file into a list, with optional filtering and limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = []\n",
        "        count = 0\n",
        "        total_processed = 0\n",
        "        rejected = 0\n",
        "\n",
        "        for line in read_zst_jsonl(file_path):\n",
        "            total_processed += 1\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                if filter_func is None or filter_func(item):\n",
        "                    data.append(item)\n",
        "                    count += 1\n",
        "                    if limit and count >= limit:\n",
        "                        break\n",
        "                else:\n",
        "                    rejected += 1\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON line: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Processed a total of {total_processed} lines, passed {count} samples, and rejected {rejected} samples\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "def is_pile_cc_data(item):\n",
        "    \"\"\"\n",
        "    Checks if the data is from Pile-CC and filters for text length less than 512.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        content = item.get('text', '')\n",
        "        if len(content) < 512:\n",
        "            return item.get('meta', {}).get('pile_set_name') == \"Pile-CC\"\n",
        "        else:\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def stream_pile_cc_data(limit=None):\n",
        "    \"\"\"\n",
        "    Streams Pile-CC data, ensuring the length is less than 512.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    pile_cc_data = []\n",
        "    total_processed = 0\n",
        "    rejected_long = 0\n",
        "\n",
        "    ds_stream = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\", streaming=True)\n",
        "\n",
        "    for item in tqdm(ds_stream, desc=\"Loading Pile-CC data\"):\n",
        "        total_processed += 1\n",
        "        content = item.get('text', '')\n",
        "\n",
        "        if item['meta']['pile_set_name'] == \"Pile-CC\" and len(content) < 512:\n",
        "            pile_cc_data.append(item)\n",
        "            count += 1\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "        elif item['meta']['pile_set_name'] == \"Pile-CC\":\n",
        "            rejected_long += 1\n",
        "\n",
        "    print(f\"Processed a total of {total_processed} items, filtered {len(pile_cc_data)} qualifying Pile-CC items\")\n",
        "    print(f\"Pile-CC items rejected due to length > 512 characters: {rejected_long}\")\n",
        "    return pile_cc_data\n",
        "\n",
        "def check_content_overlap(ds_train, ds_dev, ds_test):\n",
        "    \"\"\"\n",
        "    Checks for text overlap between different datasets.\n",
        "    \"\"\"\n",
        "    train_texts = set(ds_train['text'])\n",
        "    dev_texts = set(ds_dev['text'])\n",
        "    test_texts = set(ds_test['text'])\n",
        "\n",
        "    train_dev_overlap = train_texts.intersection(dev_texts)\n",
        "    train_test_overlap = train_texts.intersection(ds_test['text'])\n",
        "    dev_test_overlap   = dev_texts.intersection(ds_test['text'])\n",
        "\n",
        "    return len(train_dev_overlap), len(train_test_overlap), len(dev_test_overlap)\n",
        "\n",
        "def check_sample_lengths(dataset, name):\n",
        "    \"\"\"\n",
        "    Checks the length of all samples in the dataset.\n",
        "    \"\"\"\n",
        "    lengths = [len(text) for text in dataset['text']]\n",
        "    max_length = max(lengths)\n",
        "    min_length = min(lengths)\n",
        "    avg_length = sum(lengths) / len(lengths)\n",
        "\n",
        "    over_512 = sum(1 for l in lengths if l >= 512)\n",
        "\n",
        "    print(f\"\\n{name} Length Check:\")\n",
        "    print(f\"Number of samples: {len(dataset)}\")\n",
        "    print(f\"Min length: {min_length}, Max length: {max_length}, Avg length: {avg_length:.1f}\")\n",
        "    print(f\"Samples >= 512 characters: {over_512}/{len(dataset)} ({(over_512/len(dataset))*100:.1f}%)\")\n",
        "\n",
        "    # Check the length distribution for label=0 and label=1 samples respectively\n",
        "    lengths_0 = [len(dataset['text'][i]) for i in range(len(dataset)) if dataset['label'][i] == 0]\n",
        "    lengths_1 = [len(dataset['text'][i]) for i in range(len(dataset)) if dataset['label'][i] == 1]\n",
        "\n",
        "    print(f\"Label 0 samples: Count={len(lengths_0)}, Avg length={sum(lengths_0)/max(1,len(lengths_0)):.1f}\")\n",
        "    print(f\"Label 1 samples: Count={len(lengths_1)}, Avg length={sum(lengths_1)/max(1,len(lengths_1)):.1f}\")\n",
        "\n",
        "    return lengths\n",
        "\n",
        "def filter_text_by_length(text, max_length=512):\n",
        "    \"\"\"\n",
        "    Filters text, keeping samples with length less than a specified value.\n",
        "    \"\"\"\n",
        "    return len(text) < max_length\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Part 2: Text Rewriting Using BERT Model\n",
        "###############################################################################\n",
        "\n",
        "# Initialize BERT model\n",
        "print(\"\\n== Initializing BERT model for text rewriting ==\")\n",
        "model_name = \"bert-base-uncased\"  # Can be replaced with other BERT variants, e.g., \"bert-large-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "def bert_rewrite_text(text, mask_prob=0.15, max_length=512):\n",
        "    \"\"\"\n",
        "    Rewrites text using the BERT model.\n",
        "    Generates slightly different text by randomly masking some words and having BERT predict them.\n",
        "    \"\"\"\n",
        "    # Ensure the text is not empty\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        return text\n",
        "\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get token IDs\n",
        "    input_ids = inputs[\"input_ids\"].clone()\n",
        "\n",
        "    # Create a random mask\n",
        "    # Exclude [CLS], [SEP] and other special tokens\n",
        "    special_tokens_mask = tokenizer.get_special_tokens_mask(input_ids[0], already_has_special_tokens=True)\n",
        "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "\n",
        "    # Randomly select tokens to mask\n",
        "    probability_matrix = torch.full(input_ids.shape, mask_prob)\n",
        "    probability_matrix[0][special_tokens_mask] = 0.0\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "\n",
        "    # Remember the original masked tokens\n",
        "    original_masked_tokens = input_ids[masked_indices]\n",
        "\n",
        "    # Apply the mask\n",
        "    input_ids[masked_indices] = tokenizer.mask_token_id\n",
        "\n",
        "    # Use the model to predict the masked tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "\n",
        "    # Get the prediction results\n",
        "    predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "    # Replace the masked positions with the predicted tokens\n",
        "    input_ids[masked_indices] = predictions[masked_indices]\n",
        "\n",
        "    # Decode back to text\n",
        "    rewritten_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return rewritten_text\n",
        "\n",
        "def prompt_rewrite_batch(batch):\n",
        "    \"\"\"\n",
        "    Batch rewrite text using the BERT model.\n",
        "    \"\"\"\n",
        "    new_texts = []\n",
        "    for text in batch[\"text\"]:\n",
        "        rewrite = bert_rewrite_text(text)\n",
        "        new_texts.append(rewrite)\n",
        "    return {\"text\": new_texts}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Part 3: Main Workflow\n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    data_dir = \"/content/drive/MyDrive/LLM_MIA/data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # 1) Load CC News data\n",
        "        print(\"Starting to load CC News dataset...\")\n",
        "        cc_news = load_dataset(\"vblagoje/cc_news\")\n",
        "        print(f\"Loading complete. CC News dataset size: {len(cc_news['train'])}\")\n",
        "\n",
        "        # 2) Filter CC News for texts with length < 512\n",
        "        filtered_cc_news_texts = []\n",
        "        for item in tqdm(cc_news['train'], desc=\"Filtering CC News data\"):\n",
        "            if len(item['text']) < 512:\n",
        "                filtered_cc_news_texts.append(item['text'])\n",
        "\n",
        "        print(f\"Filtered CC News data size: {len(filtered_cc_news_texts)}\")\n",
        "        if len(filtered_cc_news_texts) < 1000:\n",
        "            raise ValueError(f\"Insufficient CC News data. Found {len(filtered_cc_news_texts)} records, but at least 1000 are required.\")\n",
        "\n",
        "        # 3) Load Pile-CC data (limit=1000)\n",
        "        pile_cc_data = stream_pile_cc_data(limit=1000)\n",
        "        pile_cc_texts = [item['text'] for item in pile_cc_data]\n",
        "        print(f\"Pile-CC data size: {len(pile_cc_texts)}\")\n",
        "\n",
        "        random.seed(42)\n",
        "\n",
        "        # 4) Split the datasets\n",
        "        random.shuffle(filtered_cc_news_texts)\n",
        "        train_size = 200\n",
        "        dev_nonmember_size = 200\n",
        "        test_nonmember_size = 400\n",
        "\n",
        "        required_cc_news = train_size + dev_nonmember_size + test_nonmember_size\n",
        "        if len(filtered_cc_news_texts) < required_cc_news:\n",
        "            raise ValueError(f\"Insufficient CC News data. Need {required_cc_news} records, but only have {len(filtered_cc_news_texts)}.\")\n",
        "\n",
        "        train_cc_news = filtered_cc_news_texts[:train_size]\n",
        "        dev_nonmember = filtered_cc_news_texts[train_size:train_size+dev_nonmember_size]\n",
        "        test_nonmember = filtered_cc_news_texts[train_size+dev_nonmember_size : train_size+dev_nonmember_size+test_nonmember_size]\n",
        "\n",
        "        # Pile-CC split\n",
        "        dev_member_size = 200\n",
        "        test_member_size = 400\n",
        "        required_pile_cc = dev_member_size + test_member_size\n",
        "        if len(pile_cc_texts) < required_pile_cc:\n",
        "            raise ValueError(f\"Insufficient Pile-CC data. Need {required_pile_cc} records, but only have {len(pile_cc_texts)}.\")\n",
        "\n",
        "        random.shuffle(pile_cc_texts)\n",
        "        dev_member  = pile_cc_texts[:dev_member_size]\n",
        "        test_member = pile_cc_texts[dev_member_size : dev_member_size+test_member_size]\n",
        "\n",
        "        # Build train: 200 records -> 100(member)+100(nonmember)\n",
        "        random.shuffle(train_cc_news)\n",
        "        train_member = train_cc_news[: train_size//2]\n",
        "        train_nonmember = train_cc_news[train_size//2 : train_size]\n",
        "\n",
        "        ds_train = Dataset.from_dict({\n",
        "            'text':  train_member + train_nonmember,\n",
        "            'label': [1]*(train_size//2) + [0]*(train_size//2)\n",
        "        })\n",
        "\n",
        "        # Build dev: 200(member)+200(nonmember)\n",
        "        ds_dev = Dataset.from_dict({\n",
        "            'text':  dev_member + dev_nonmember,\n",
        "            'label': [1]*dev_member_size + [0]*dev_nonmember_size\n",
        "        })\n",
        "\n",
        "        # Build test: 400(member)+400(nonmember)\n",
        "        ds_test = Dataset.from_dict({\n",
        "            'text':  test_member + test_nonmember,\n",
        "            'label': [1]*test_member_size + [0]*test_nonmember_size\n",
        "        })\n",
        "\n",
        "        # ========== Check for dataset overlap and print info ==========\n",
        "        train_dev_overlap, train_test_overlap, dev_test_overlap = check_content_overlap(ds_train, ds_dev, ds_test)\n",
        "        print(\"\\nDataset Overlap Check:\")\n",
        "        print(f\"Train-Dev overlap: {train_dev_overlap}\")\n",
        "        print(f\"Train-Test overlap: {train_test_overlap}\")\n",
        "        print(f\"Dev-Test overlap: {dev_test_overlap}\")\n",
        "        if train_dev_overlap > 0 or train_test_overlap > 0 or dev_test_overlap > 0:\n",
        "            print(\"Warning: Overlap detected between datasets!\")\n",
        "\n",
        "        check_sample_lengths(ds_train, \"Training Set\")\n",
        "        check_sample_lengths(ds_dev,   \"Development Set\")\n",
        "        check_sample_lengths(ds_test,  \"Test Set\")\n",
        "\n",
        "        print(\"\\nDataset Sizes:\")\n",
        "        print(f\"Train size: {len(ds_train)}\")\n",
        "        print(f\"Dev size:   {len(ds_dev)}\")\n",
        "        print(f\"Test size:  {len(ds_test)}\")\n",
        "\n",
        "        # ============== Keep original text for comparison\n",
        "        ds_train = ds_train.add_column(\"original_text\", ds_train[\"text\"])\n",
        "        ds_dev   = ds_dev.add_column(\"original_text\", ds_dev[\"text\"])\n",
        "        ds_test  = ds_test.add_column(\"original_text\", ds_test[\"text\"])\n",
        "\n",
        "        #####################################################################\n",
        "        # Part 5: Rewriting with BERT Model\n",
        "        # Rewrite all of Train; Rewrite only non-members (label=0) for Dev/Test\n",
        "        #####################################################################\n",
        "\n",
        "        print(\"\\n1) Rewriting the training set (members + non-members)...\")\n",
        "        ds_train = ds_train.map(\n",
        "            prompt_rewrite_batch,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            desc=\"Rewriting train\"\n",
        "        )\n",
        "\n",
        "        print(\"\\n2) Rewriting the development set (non-members only)...\")\n",
        "        ds_dev_member    = ds_dev.filter(lambda x: x[\"label\"] == 1)\n",
        "        ds_dev_nonmember = ds_dev.filter(lambda x: x[\"label\"] == 0)\n",
        "\n",
        "        ds_dev_nonmember = ds_dev_nonmember.map(\n",
        "            prompt_rewrite_batch,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            desc=\"Rewriting dev non-members\"\n",
        "        )\n",
        "\n",
        "        # Combine and shuffle\n",
        "        ds_dev = concatenate_datasets([ds_dev_member, ds_dev_nonmember]).shuffle(seed=42)\n",
        "\n",
        "        print(\"\\n3) Rewriting the test set (non-members only)...\")\n",
        "        ds_test_member    = ds_test.filter(lambda x: x[\"label\"] == 1)\n",
        "        ds_test_nonmember = ds_test.filter(lambda x: x[\"label\"] == 0)\n",
        "\n",
        "        ds_test_nonmember = ds_test_nonmember.map(\n",
        "            prompt_rewrite_batch,\n",
        "            batched=True,\n",
        "            batch_size=16,\n",
        "            desc=\"Rewriting test non-members\"\n",
        "        )\n",
        "\n",
        "        ds_test = concatenate_datasets([ds_test_member, ds_test_nonmember]).shuffle(seed=42)\n",
        "\n",
        "        #####################################################################\n",
        "        # Part 6: Reviewing Examples for Comparison\n",
        "        #####################################################################\n",
        "        print(\"\\n==== Reviewing before-and-after examples (Training Set) ====\")\n",
        "        for _ in range(3):\n",
        "            idx = random.randint(0, len(ds_train)-1)\n",
        "            orig = ds_train[idx][\"original_text\"]\n",
        "            newt = ds_train[idx][\"text\"]\n",
        "            lab  = ds_train[idx][\"label\"]\n",
        "            print(f\"[Train idx={idx}, label={lab}]\")\n",
        "            print(\"Original:      \", orig)\n",
        "            print(\"After Rewrite: \", newt)\n",
        "            print(\"----\")\n",
        "\n",
        "        print(\"\\n==== Reviewing before-and-after examples (Development Set) ====\")\n",
        "        for _ in range(3):\n",
        "            idx = random.randint(0, len(ds_dev)-1)\n",
        "            orig = ds_dev[idx][\"original_text\"]\n",
        "            newt = ds_dev[idx][\"text\"]\n",
        "            lab  = ds_dev[idx][\"label\"]\n",
        "            print(f\"[Dev idx={idx}, label={lab}]\")\n",
        "            print(\"Original:      \", orig)\n",
        "            print(\"After Rewrite: \", newt)\n",
        "            print(\"----\")\n",
        "\n",
        "        print(\"\\n==== Reviewing before-and-after examples (Test Set) ====\")\n",
        "        for _ in range(3):\n",
        "            idx = random.randint(0, len(ds_test)-1)\n",
        "            orig = ds_test[idx][\"original_text\"]\n",
        "            newt = ds_test[idx][\"text\"]\n",
        "            lab  = ds_test[idx][\"label\"]\n",
        "            print(f\"[Test idx={idx}, label={lab}]\")\n",
        "            print(\"Original:      \", orig)\n",
        "            print(\"After Rewrite: \", newt)\n",
        "            print(\"----\")\n",
        "\n",
        "        #####################################################################\n",
        "        # Part 7: Saving the Rewritten Datasets\n",
        "        #####################################################################\n",
        "        ds_train.save_to_disk(os.path.join(data_dir, \"pile_cc_mia_train_bert\"))\n",
        "        ds_dev.save_to_disk(os.path.join(data_dir, \"pile_cc_mia_dev_bert\"))\n",
        "        ds_test.save_to_disk(os.path.join(data_dir, \"pile_cc_mia_test_bert\"))\n",
        "\n",
        "        print(\"\\nRewritten datasets have been successfully saved to:\")\n",
        "        print(\" -\", os.path.join(data_dir, \"pile_cc_mia_train_bert\"))\n",
        "        print(\" -\", os.path.join(data_dir, \"pile_cc_mia_dev_bert\"))\n",
        "        print(\" -\", os.path.join(data_dir, \"pile_cc_mia_test_bert\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "###############################################################################\n",
        "# Run the main function\n",
        "###############################################################################\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8sEzDkhT7FT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "data_dir = \"/content/drive/MyDrive/LLM_MIA/data\"\n",
        "\n",
        "ds_train = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_train_bert\"))\n",
        "ds_dev = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_dev_bert\"))\n",
        "ds_test = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_test_bert\"))\n",
        "\n",
        "print(f\"train size: {len(ds_train)}\")\n",
        "print(f\"dev size: {len(ds_dev)}\")\n",
        "print(f\"test size: {len(ds_test)}\")"
      ],
      "metadata": {
        "id": "IDoPC217QuWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Prompt"
      ],
      "metadata": {
        "id": "dMkKbxJ13KEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "data_dir = \"/content/drive/MyDrive/LLM_MIA/data\"\n",
        "\n",
        "ds_train = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_train_prompt_gpt4o_simple_v1\"))\n",
        "ds_dev = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_dev_prompt_gpt4o_simple_v1\"))\n",
        "ds_test = load_from_disk(os.path.join(data_dir, \"pile_cc_mia_test_prompt_gpt4o_simple_v1\"))\n",
        "\n",
        "print(f\"train size: {len(ds_train)}\")\n",
        "print(f\"dev size: {len(ds_dev)}\")\n",
        "print(f\"test size: {len(ds_test)}\")\n"
      ],
      "metadata": {
        "id": "ZCuUe4nsQel8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WikiMIA"
      ],
      "metadata": {
        "id": "AhX9c4qb6BaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"swj0419/WikiMIA\")\n",
        "\n",
        "# Select only the length32 data\n",
        "length32_data = ds[\"WikiMIA_length32\"]\n",
        "\n",
        "# Separate member (label=1) and non-member (label=0) samples, and rename the 'input' field to 'text'\n",
        "member_samples = []\n",
        "nonmember_samples = []\n",
        "\n",
        "for item in length32_data:\n",
        "    # Create a new sample dictionary, changing the 'input' field to 'text'\n",
        "    new_item = {\n",
        "        'text': item['input'],\n",
        "        'label': item['label']\n",
        "    }\n",
        "    # Keep other existing fields\n",
        "    for key, value in item.items():\n",
        "        if key not in ['input', 'text', 'label']:\n",
        "            new_item[key] = value\n",
        "\n",
        "    # Classify based on the label\n",
        "    if item['label'] == 1:\n",
        "        member_samples.append(new_item)\n",
        "    else:\n",
        "        nonmember_samples.append(new_item)\n",
        "\n",
        "print(f\"Number of member samples: {len(member_samples)}\")\n",
        "print(f\"Number of non-member samples: {len(nonmember_samples)}\")\n",
        "\n",
        "# Adjust dataset sizes\n",
        "# Use the number of available samples to determine the split\n",
        "available_member_count = len(member_samples)\n",
        "available_nonmember_count = len(nonmember_samples)\n",
        "\n",
        "# Set the number of member samples in the dev and test sets\n",
        "# Considering there are only 387 member samples, we can take 140 for dev and the remaining 247 for test\n",
        "dev_member_count = 140\n",
        "test_member_count = available_member_count - dev_member_count  # should be 247\n",
        "\n",
        "# Set the same number of non-member samples to maintain balance\n",
        "dev_nonmember_count = dev_member_count\n",
        "test_nonmember_count = test_member_count\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split the member samples\n",
        "dev_members = member_samples[:dev_member_count]\n",
        "test_members = member_samples[dev_member_count:]\n",
        "\n",
        "# Split the non-member samples\n",
        "dev_nonmembers = nonmember_samples[:dev_nonmember_count]\n",
        "test_nonmembers = nonmember_samples[dev_nonmember_count:dev_nonmember_count+test_nonmember_count]\n",
        "\n",
        "# Combine the dev set\n",
        "ds_dev = dev_members + dev_nonmembers\n",
        "np.random.shuffle(ds_dev)  # Shuffle the order\n",
        "\n",
        "# Combine the test set\n",
        "ds_test = test_members + test_nonmembers\n",
        "np.random.shuffle(ds_test)  # Shuffle the order\n",
        "\n",
        "# Verify dataset sizes and label balance\n",
        "print(f\"Dev set size: {len(ds_dev)}\")\n",
        "print(f\"Number of member samples in Dev set: {sum(1 for item in ds_dev if item['label'] == 1)}\")\n",
        "print(f\"Number of non-member samples in Dev set: {sum(1 for item in ds_dev if item['label'] == 0)}\")\n",
        "\n",
        "print(f\"Test set size: {len(ds_test)}\")\n",
        "print(f\"Number of member samples in Test set: {sum(1 for item in ds_test if item['label'] == 1)}\")\n",
        "print(f\"Number of non-member samples in Test set: {sum(1 for item in ds_test if item['label'] == 0)}\")\n",
        "\n",
        "# Save the datasets\n",
        "ds_dev = Dataset.from_list(ds_dev)\n",
        "ds_test = Dataset.from_list(ds_test)\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Save to disk\n",
        "ds_dev.save_to_disk(\"data/ds_dev\")\n",
        "ds_test.save_to_disk(\"data/ds_test\")\n",
        "\n",
        "print(\"Datasets have been saved to the data/ds_dev and data/ds_test directories\")"
      ],
      "metadata": {
        "id": "zhGBtGBr6Cfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ArxivMia"
      ],
      "metadata": {
        "id": "WTfnJzg3XdPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/data'\n",
        "cache_dir = '/content/drive/MyDrive/cache_dir'\n",
        "\n",
        "def check_content_overlap(ds_train, ds_dev, ds_test):\n",
        "    train_texts = set(ds_train['text'])\n",
        "    dev_texts   = set(ds_dev['text'])\n",
        "    test_texts  = set(ds_test['text'])\n",
        "\n",
        "    train_dev_overlap = train_texts.intersection(dev_texts)\n",
        "    train_test_overlap = train_texts.intersection(test_texts)\n",
        "    dev_test_overlap   = dev_texts.intersection(test_texts)\n",
        "\n",
        "    return len(train_dev_overlap), len(train_test_overlap), len(dev_test_overlap)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Load local train data\n",
        "    ds_train = load_dataset(\n",
        "        \"json\",\n",
        "        data_files=\"/content/drive/MyDrive/LLM_MIA/arxiv_mia_train_real.jsonl\",\n",
        "        split=\"train\"\n",
        "    )\n",
        "\n",
        "    # 2. Load dev, test from Hugging Face\n",
        "    ds_dev  = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_dev\")[\"train\"]\n",
        "    ds_test = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_test\")[\"train\"]\n",
        "\n",
        "    print(\"Train size:\", len(ds_train))\n",
        "    print(\"Dev size:  \", len(ds_dev))\n",
        "    print(\"Test size: \", len(ds_test))\n",
        "\n",
        "    # 3. Check for overlap between datasets\n",
        "    train_dev_overlap, train_test_overlap, dev_test_overlap = check_content_overlap(ds_train, ds_dev, ds_test)\n",
        "    print(\"\\nChecking dataset overlap:\")\n",
        "    print(f\"Train-Dev overlap: {train_dev_overlap}\")\n",
        "    print(f\"Train-Test overlap: {train_test_overlap}\")\n",
        "    print(f\"Dev-Test overlap: {dev_test_overlap}\")"
      ],
      "metadata": {
        "id": "7wHwvXh2XQcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Ei5UlccsKKk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ArxivMia"
      ],
      "metadata": {
        "id": "gjIXBUggtW6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TinyLlama"
      ],
      "metadata": {
        "id": "MINmeuypKGzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_fscore_support\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable tqdm - a safer way\n",
        "import sys\n",
        "import importlib\n",
        "import tqdm as tqdm_module\n",
        "\n",
        "# Save the original module before loading tqdm\n",
        "original_module = sys.modules.get('tqdm', None)\n",
        "\n",
        "# Create a no-op tqdm substitute\n",
        "class DummyTqdmModule:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def update(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def close(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        raise StopIteration\n",
        "\n",
        "# Override all methods of the tqdm class\n",
        "def dummy_tqdm(*args, **kwargs):\n",
        "    if len(args) > 0 and isinstance(args[0], list):\n",
        "        return args[0]\n",
        "    return DummyTqdmModule()\n",
        "\n",
        "# Patch all possible tqdm versions\n",
        "for name in ['tqdm', 'tqdm.std', 'tqdm.auto', 'tqdm.notebook', 'tqdm.rich', 'tqdm.cli', 'tqdm.gui', 'tqdm.keras']:\n",
        "    try:\n",
        "        if name in sys.modules:\n",
        "            module = sys.modules[name]\n",
        "            module.tqdm = dummy_tqdm\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "tqdm_module.tqdm = dummy_tqdm\n",
        "\n",
        "# 1. Set up the environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3. Load the model - changed to TinyLlama\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is on the correct device\n",
        "model = model.to(device)\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# If the tokenizer does not have a padding token, set one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 4. Set up hook functions to collect neuron activations\n",
        "activations = {}\n",
        "activated_neurons = {}\n",
        "ACTIVATION_THRESHOLD = 0  # Set activation threshold\n",
        "\n",
        "def get_ffn_activation(name):\n",
        "    def hook(module, input, output):\n",
        "        if output.dim() >= 2:\n",
        "            # If it's a 3D tensor [batch_size, seq_len, hidden_dim]\n",
        "            if output.dim() == 3:\n",
        "                # Get only the indices after masking\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "            # If it's a 2D tensor [batch_size, hidden_dim]\n",
        "            elif output.dim() == 2:\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "    return hook\n",
        "\n",
        "# 5. Register hooks for the model\n",
        "hooks = []\n",
        "\n",
        "# Register hooks for the activation function in LlamaMLP\n",
        "# TinyLlama uses the Llama architecture, and the MLP part has act_fn as the activation function\n",
        "for i, layer in enumerate(model.model.layers):\n",
        "    # Register MLP activation function hook\n",
        "    if hasattr(layer.mlp, 'act_fn'):\n",
        "        # Set hook for SiLU activation function\n",
        "        hook = layer.mlp.act_fn.register_forward_hook(get_ffn_activation(f'layer_{i}_mlp_act'))\n",
        "        hooks.append(hook)\n",
        "    else:\n",
        "        # If the 'act_fn' attribute does not exist, try to find other activation layers\n",
        "        for name, module in layer.mlp.named_modules():\n",
        "            if isinstance(module, torch.nn.SiLU) or isinstance(module, torch.nn.GELU):\n",
        "                hook = module.register_forward_hook(get_ffn_activation(f'layer_{i}_mlp_act_{name}'))\n",
        "                hooks.append(hook)\n",
        "                break\n",
        "\n",
        "print(f\"Registered hooks for {len(model.model.layers)}-layer model\")\n",
        "\n",
        "# 7. Collect sample activation data - simplified version, only storing activated neuron indices\n",
        "def process_sample(sample, sample_id):\n",
        "    \"\"\"Process a single sample and collect only the index information of activated neurons\"\"\"\n",
        "    # Clear previous activations\n",
        "    activations.clear()\n",
        "    activated_neurons.clear()\n",
        "\n",
        "    # Prepare input for the sample\n",
        "    encodings = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "\n",
        "    # Run the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "\n",
        "    # Collect only the indices of activated neurons, do not store activation values\n",
        "    sample_neural_signature = {}\n",
        "    for key, value in activated_neurons.items():\n",
        "        try:\n",
        "            if len(value.shape) >= 2:\n",
        "                mask = value.squeeze(0).numpy()\n",
        "\n",
        "                # Only collect which neurons are activated in each layer, without recording position or activation value\n",
        "                if len(mask.shape) == 2:  # [seq_len, hidden_dim]\n",
        "                    # Merge the activation status across all positions\n",
        "                    # Any neuron activated at any position is considered activated\n",
        "                    active_neurons = set()\n",
        "                    for pos in range(mask.shape[0]):\n",
        "                        active_indices = np.where(mask[pos])[0]\n",
        "                        active_neurons.update(active_indices)\n",
        "\n",
        "                    if active_neurons:\n",
        "                        sample_neural_signature[key] = list(active_neurons)\n",
        "                elif len(mask.shape) == 1:  # [hidden_dim]\n",
        "                    active_indices = np.where(mask)[0]\n",
        "                    if len(active_indices) > 0:\n",
        "                        sample_neural_signature[key] = active_indices.tolist()\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    # Return sample information, containing only the label and neural signature\n",
        "    return {\n",
        "        'sample_id': sample_id,\n",
        "        'label': sample['label'],\n",
        "        'neural_signature': sample_neural_signature\n",
        "    }\n",
        "\n",
        "def collect_activations(samples, batch_size=10):\n",
        "    \"\"\"Collect activation data for samples in batches\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Process samples in batches to save memory\n",
        "    for i in range(0, len(samples), batch_size):\n",
        "        batch = samples[i:i+batch_size]\n",
        "        batch_results = []\n",
        "\n",
        "        for j, sample in enumerate(batch):\n",
        "            try:\n",
        "                result = process_sample(sample, i + j)\n",
        "                batch_results.append(result)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "        # Clean up memory\n",
        "        if i + batch_size < len(samples):  # Not the last batch\n",
        "            del batch_results\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# 8. Analyze neuron activation patterns - simplified version, only analyzing neuron activation indices\n",
        "def analyze_neuron_activation_patterns(member_samples, nonmember_samples):\n",
        "    \"\"\"Analyze the activation patterns of neurons in member and non-member samples\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Get all layer names\n",
        "    layer_names = set()\n",
        "    for sample in member_samples + nonmember_samples:\n",
        "        layer_names.update(sample['neural_signature'].keys())\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in layer_names:\n",
        "        # Count activated neurons in member and non-member samples\n",
        "        member_neuron_counts = Counter()\n",
        "        nonmember_neuron_counts = Counter()\n",
        "\n",
        "        # Activated neurons in member samples\n",
        "        for sample in member_samples:\n",
        "            if layer_name in sample['neural_signature']:\n",
        "                member_neuron_counts.update(sample['neural_signature'][layer_name])\n",
        "\n",
        "        # Activated neurons in non-member samples\n",
        "        for sample in nonmember_samples:\n",
        "            if layer_name in sample['neural_signature']:\n",
        "                nonmember_neuron_counts.update(sample['neural_signature'][layer_name])\n",
        "\n",
        "        # Calculate the activation frequency for each neuron\n",
        "        member_freq = {n: count / len(member_samples) for n, count in member_neuron_counts.items()}\n",
        "        nonmember_freq = {n: count / len(nonmember_samples) for n, count in nonmember_neuron_counts.items()}\n",
        "\n",
        "        # Identify neurons predominantly activated in member samples\n",
        "        member_dominant = {}\n",
        "        for neuron, freq in member_freq.items():\n",
        "            if neuron not in nonmember_freq or freq > nonmember_freq[neuron] * 1.5:\n",
        "                member_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons predominantly activated in non-member samples\n",
        "        nonmember_dominant = {}\n",
        "        for neuron, freq in nonmember_freq.items():\n",
        "            if neuron not in member_freq or freq > member_freq[neuron] * 1.5:\n",
        "                nonmember_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons frequently activated in both types of samples\n",
        "        common_neurons = {}\n",
        "        for neuron in set(member_freq.keys()) & set(nonmember_freq.keys()):\n",
        "            if neuron not in member_dominant and neuron not in nonmember_dominant:\n",
        "                common_neurons[neuron] = (member_freq[neuron], nonmember_freq[neuron])\n",
        "\n",
        "        # Save the results\n",
        "        results[layer_name] = {\n",
        "            'member_dominant': member_dominant,\n",
        "            'nonmember_dominant': nonmember_dominant,\n",
        "            'common_neurons': common_neurons,\n",
        "            'member_freq': member_freq,\n",
        "            'nonmember_freq': nonmember_freq\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# 9. Build reference patterns\n",
        "def build_reference_patterns(train_samples):\n",
        "    \"\"\"Build reference activation patterns using training samples\"\"\"\n",
        "    # Separate member and non-member samples\n",
        "    member_samples = [s for s in train_samples if s['label'] == 1]\n",
        "    nonmember_samples = [s for s in train_samples if s['label'] == 0]\n",
        "\n",
        "    print(f\"Building reference patterns: {len(member_samples)} member samples, {len(nonmember_samples)} non-member samples\")\n",
        "\n",
        "    # Analyze activation pattern differences\n",
        "    reference_patterns = analyze_neuron_activation_patterns(member_samples, nonmember_samples)\n",
        "\n",
        "    return reference_patterns\n",
        "\n",
        "# 10. Calculate discrimination score for each layer\n",
        "def calculate_layer_discrimination_scores(reference_patterns):\n",
        "    \"\"\"Calculate the discriminative power score for each layer\"\"\"\n",
        "    layer_scores = {}\n",
        "\n",
        "    # Calculate the discriminative power score for each layer\n",
        "    for layer_name, data in reference_patterns.items():\n",
        "        # Simple discrimination score: number of member-dominant neurons minus number of non-member-dominant neurons\n",
        "        # If the score is greater than 0, it indicates the layer is more inclined to identify member samples\n",
        "        member_dominant_count = len(data['member_dominant'])\n",
        "        nonmember_dominant_count = len(data['nonmember_dominant'])\n",
        "\n",
        "        discrimination_score = member_dominant_count - nonmember_dominant_count\n",
        "        layer_scores[layer_name] = discrimination_score\n",
        "\n",
        "    return layer_scores\n",
        "\n",
        "# 11. Select the most discriminative layers\n",
        "def select_discriminative_layers(layer_scores, top_n=10):\n",
        "    \"\"\"Select the most discriminative layers\"\"\"\n",
        "    # Sort by discrimination score\n",
        "    sorted_scores = sorted(layer_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    # Select the top N layers\n",
        "    selected_layers = [layer for layer, score in sorted_scores[:top_n]]\n",
        "\n",
        "    return selected_layers\n",
        "\n",
        "# 12. Membership prediction based on relative ratio - using simplified neural signatures\n",
        "def predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers, threshold=1.0):\n",
        "    \"\"\"Predict whether a single sample is a member using the relative ratio method\"\"\"\n",
        "    if not sample['neural_signature']:\n",
        "        return 0, 0, 0  # If there is no activation data, default to non-member\n",
        "\n",
        "    # Initialize relative ratios\n",
        "    layers_counted = 0\n",
        "    total_member_ratio = 0\n",
        "    total_nonmember_ratio = 0\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in discriminative_layers:\n",
        "        if layer_name not in sample['neural_signature'] or layer_name not in reference_patterns:\n",
        "            continue\n",
        "\n",
        "        # Get the reference pattern for this layer\n",
        "        layer_data = reference_patterns[layer_name]\n",
        "\n",
        "        # Get all neurons activated by this sample in this layer\n",
        "        sample_neurons = set(sample['neural_signature'][layer_name])\n",
        "\n",
        "        if not sample_neurons:\n",
        "            continue\n",
        "\n",
        "        # Calculate the relative overlap with member-dominant neurons\n",
        "        member_dominant_set = set(layer_data['member_dominant'].keys())\n",
        "        member_overlap = len(sample_neurons.intersection(member_dominant_set))\n",
        "\n",
        "        # Calculate the relative overlap with non-member-dominant neurons\n",
        "        nonmember_dominant_set = set(layer_data['nonmember_dominant'].keys())\n",
        "        nonmember_overlap = len(sample_neurons.intersection(nonmember_dominant_set))\n",
        "\n",
        "        # Calculate relative ratios\n",
        "        member_ratio = member_overlap / len(member_dominant_set) if len(member_dominant_set) > 0 else 0\n",
        "        nonmember_ratio = nonmember_overlap / len(nonmember_dominant_set) if len(nonmember_dominant_set) > 0 else 0\n",
        "\n",
        "        # Accumulate relative ratios\n",
        "        total_member_ratio += member_ratio\n",
        "        total_nonmember_ratio += nonmember_ratio\n",
        "        layers_counted += 1\n",
        "\n",
        "    # If there are no valid layers, default to non-member\n",
        "    if layers_counted == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    # Calculate average relative ratios\n",
        "    avg_member_ratio = total_member_ratio / layers_counted\n",
        "    avg_nonmember_ratio = total_nonmember_ratio / layers_counted\n",
        "\n",
        "    # Calculate the ratio of relative ratios\n",
        "    if avg_nonmember_ratio == 0:\n",
        "        ratio = float('inf')  # Avoid division by zero\n",
        "    else:\n",
        "        ratio = avg_member_ratio / avg_nonmember_ratio\n",
        "\n",
        "    # Return the prediction and related ratios\n",
        "    return 1 if ratio >= threshold else 0, avg_member_ratio, avg_nonmember_ratio\n",
        "\n",
        "# 14. Find the best threshold on the validation set\n",
        "def find_best_relative_ratio_threshold(val_samples, reference_patterns, discriminative_layers):\n",
        "    \"\"\"Find the best threshold on the validation set using the relative ratio method\"\"\"\n",
        "    # Calculate relative ratios for all validation samples\n",
        "    ratios = []\n",
        "    labels = []\n",
        "\n",
        "    for sample in val_samples:\n",
        "        pred, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "            sample, reference_patterns, discriminative_layers, threshold=1.0\n",
        "        )\n",
        "\n",
        "        # Calculate the ratio of relative ratios\n",
        "        ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "        ratios.append(ratio)\n",
        "        labels.append(sample['label'])\n",
        "\n",
        "    # Filter out infinite values\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for r, l in zip(ratios, labels):\n",
        "        if r != float('inf') and not np.isnan(r):\n",
        "            filtered_ratios.append(r)\n",
        "            filtered_labels.append(l)\n",
        "\n",
        "    # Check if there are valid ratio values\n",
        "    if not filtered_ratios:\n",
        "        print(\"Warning: No valid ratio values found. Using default threshold 1.0\")\n",
        "        return 1.0, {\n",
        "            'accuracy': 0.5,  # Default accuracy\n",
        "            'precision': 0.0,\n",
        "            'recall': 0.0,\n",
        "            'f1': 0.0\n",
        "        }\n",
        "\n",
        "    # Create candidate thresholds\n",
        "    min_ratio = min(filtered_ratios)\n",
        "    max_ratio = max(filtered_ratios)\n",
        "\n",
        "    # Generate uniformly distributed thresholds\n",
        "    candidate_thresholds = list(np.linspace(min_ratio, max_ratio, 100))\n",
        "    # Add some important threshold points\n",
        "    candidate_thresholds += [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n",
        "    candidate_thresholds = sorted(set(candidate_thresholds))\n",
        "\n",
        "    # Find the best threshold\n",
        "    best_threshold = 1.0\n",
        "    best_accuracy = 0\n",
        "    best_metrics = None\n",
        "\n",
        "    results = []\n",
        "    for threshold in candidate_thresholds:\n",
        "        # Make predictions using the current threshold\n",
        "        predictions = [1 if r >= threshold else 0 for r in ratios]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "\n",
        "        # Record the results\n",
        "        results.append({\n",
        "            'threshold': threshold,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        # Update the best threshold\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Calculate AUC for the test set\n",
        "def calculate_auc(test_samples, reference_patterns, discriminative_layers, batch_size=10):\n",
        "    \"\"\"Calculate the AUC value for the test set\"\"\"\n",
        "    # Process test samples in batches\n",
        "    all_ratios = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(test_samples), batch_size):\n",
        "        batch = test_samples[i:i+batch_size]\n",
        "\n",
        "        # Collect activation data\n",
        "        batch_acts = collect_activations(batch, batch_size=batch_size)\n",
        "\n",
        "        for sample in batch_acts:\n",
        "            # Calculate relative ratios and predict\n",
        "            _, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "                sample, reference_patterns, discriminative_layers\n",
        "            )\n",
        "\n",
        "            # Calculate the ratio of relative ratios\n",
        "            ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "            # Record the results\n",
        "            all_ratios.append(ratio)\n",
        "            all_labels.append(sample['label'])\n",
        "\n",
        "        # Release memory\n",
        "        del batch_acts\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Filter out infinite values for ROC analysis\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for ratio, label in zip(all_ratios, all_labels):\n",
        "        if ratio != float('inf') and not np.isnan(ratio):\n",
        "            filtered_ratios.append(ratio)\n",
        "            filtered_labels.append(label)\n",
        "\n",
        "    # Handle the case of no valid ratios\n",
        "    if not filtered_ratios:\n",
        "        print(\"Warning: No valid ratio values found. Cannot calculate AUC.\")\n",
        "        return 0.5  # Return a default value, equivalent to the AUC of a random guess\n",
        "\n",
        "    # Calculate AUC\n",
        "    fpr, tpr, _ = roc_curve(filtered_labels, filtered_ratios)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "# Simply iterate over samples without using tqdm\n",
        "def iterate_samples(samples):\n",
        "    \"\"\"Simply iterate over samples, avoiding tqdm\"\"\"\n",
        "    return samples\n",
        "\n",
        "# Dataset filtering functions\n",
        "def filter_math_papers(dataset):\n",
        "    \"\"\"Filter dataset to only include math papers\"\"\"\n",
        "    return dataset.filter(lambda x: x['field'] == 'math')\n",
        "\n",
        "def filter_cs_papers(dataset):\n",
        "    \"\"\"Filter dataset to only include CS papers\"\"\"\n",
        "    return dataset.filter(lambda x: x['field'] == 'cs')\n",
        "\n",
        "# Process all thresholds for a single dataset\n",
        "def process_dataset(data_type, data_dir=\"/content/drive/MyDrive/LLM_MIA/data\", thresholds=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]):\n",
        "    \"\"\"Process all thresholds for a single dataset\"\"\"\n",
        "    global ACTIVATION_THRESHOLD\n",
        "\n",
        "    print(f\"\\n===== Dataset: {data_type} =====\")\n",
        "\n",
        "    # Load dataset - changed to Arxiv dataset\n",
        "    if data_type == \"arxiv_all\":\n",
        "        # 1. Load local training data\n",
        "        ds_train = load_dataset(\n",
        "            \"json\",\n",
        "            data_files=\"/content/drive/MyDrive/LLM_MIA/arxiv_mia_train_real.jsonl\",\n",
        "            split=\"train\"\n",
        "        )\n",
        "        # 2. Load dev and test from huggingface\n",
        "        ds_dev = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_dev\")[\"train\"]\n",
        "        ds_test = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_test\")[\"train\"]\n",
        "\n",
        "    elif data_type == \"arxiv_math\":\n",
        "        # 1. Load local training data and filter for math papers\n",
        "        ds_train = load_dataset(\n",
        "            \"json\",\n",
        "            data_files=\"/content/drive/MyDrive/LLM_MIA/arxiv_mia_train_real.jsonl\",\n",
        "            split=\"train\"\n",
        "        )\n",
        "        ds_train = filter_math_papers(ds_train)\n",
        "\n",
        "        # 2. Load dev and test from huggingface and filter for math papers\n",
        "        ds_dev = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_dev\")[\"train\"]\n",
        "        ds_test = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_test\")[\"train\"]\n",
        "\n",
        "        ds_dev = filter_math_papers(ds_dev)\n",
        "        ds_test = filter_math_papers(ds_test)\n",
        "\n",
        "    elif data_type == \"arxiv_cs\":\n",
        "        # 1. Load local training data and filter for CS papers\n",
        "        ds_train = load_dataset(\n",
        "            \"json\",\n",
        "            data_files=\"/content/drive/MyDrive/LLM_MIA/arxiv_mia_train_real.jsonl\",\n",
        "            split=\"train\"\n",
        "        )\n",
        "        ds_train = filter_cs_papers(ds_train)\n",
        "\n",
        "        # 2. Load dev and test from huggingface and filter for CS papers\n",
        "        ds_dev = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_dev\")[\"train\"]\n",
        "        ds_test = load_dataset(\"zhliu/ArxivMIA\", \"arxiv_mia_test\")[\"train\"]\n",
        "\n",
        "        ds_dev = filter_cs_papers(ds_dev)\n",
        "        ds_test = filter_cs_papers(ds_test)\n",
        "\n",
        "    # Process for each threshold\n",
        "    for threshold in thresholds:\n",
        "        # Set the current threshold\n",
        "        ACTIVATION_THRESHOLD = threshold\n",
        "\n",
        "        # 1. Get member and non-member samples from the development set\n",
        "        dev_member = [item for item in ds_dev if item['label'] == 1]\n",
        "        dev_nonmember = [item for item in ds_dev if item['label'] == 0]\n",
        "\n",
        "        # 2. Split the development set evenly into training and validation sets\n",
        "        train_member = dev_member[:len(dev_member)//2]\n",
        "        train_nonmember = dev_nonmember[:len(dev_nonmember)//2]\n",
        "\n",
        "        val_member = dev_member[len(dev_member)//2:]\n",
        "        val_nonmember = dev_nonmember[len(dev_nonmember)//2:]\n",
        "\n",
        "        # 3. Collect activation data\n",
        "        train_member_acts = collect_activations(train_member)\n",
        "        train_nonmember_acts = collect_activations(train_nonmember)\n",
        "\n",
        "        val_member_acts = collect_activations(val_member)\n",
        "        val_nonmember_acts = collect_activations(val_nonmember)\n",
        "\n",
        "        # 4. Build reference activation patterns\n",
        "        reference_patterns = build_reference_patterns(\n",
        "            train_member_acts + train_nonmember_acts\n",
        "        )\n",
        "\n",
        "        # 5. Calculate layer discrimination scores\n",
        "        layer_scores = calculate_layer_discrimination_scores(reference_patterns)\n",
        "\n",
        "        # 6. Select the most discriminative layers\n",
        "        discriminative_layers = select_discriminative_layers(layer_scores, top_n=10)\n",
        "\n",
        "        # 7. Find the best threshold on the validation set using the relative ratio method\n",
        "        val_samples = val_member_acts + val_nonmember_acts\n",
        "        best_threshold, val_metrics = find_best_relative_ratio_threshold(\n",
        "            val_samples,\n",
        "            reference_patterns,\n",
        "            discriminative_layers\n",
        "        )\n",
        "\n",
        "        # 8. Calculate test set AUC\n",
        "        test_auc = calculate_auc(list(ds_test), reference_patterns, discriminative_layers)\n",
        "\n",
        "        # 9. Print only the dataset, threshold, and test set AUC value\n",
        "        print(f\"Dataset: {data_type}, Threshold: {threshold:.1f}, Test Set AUC = {test_auc:.4f}\")\n",
        "\n",
        "        # 10. Clean up memory\n",
        "        del train_member_acts, train_nonmember_acts, val_member_acts, val_nonmember_acts\n",
        "        del reference_patterns, layer_scores, discriminative_layers, val_samples\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Release memory after processing the current dataset\n",
        "    del ds_train, ds_dev, ds_test\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Set the activation thresholds to be evaluated\n",
        "        activation_thresholds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
        "\n",
        "        # Process the three datasets sequentially\n",
        "        datasets = [\"arxiv_all\"]\n",
        "\n",
        "        for dataset in datasets:\n",
        "            process_dataset(dataset, thresholds=activation_thresholds)\n",
        "\n",
        "        # After all datasets are processed, remove the hooks\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {str(e)}\")\n",
        "        # Ensure hooks are removed\n",
        "        for hook in hooks:\n",
        "            try:\n",
        "                hook.remove()\n",
        "            except:\n",
        "                pass\n",
        "        raise e"
      ],
      "metadata": {
        "id": "Vdl-odS8tZio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open_Llama_13b"
      ],
      "metadata": {
        "id": "EOXGQlwWKd3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy import stats\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
        "import tqdm\n",
        "from transformers import BitsAndBytesConfig\n",
        "import gc\n",
        "\n",
        "# Define a function that does nothing and returns its input\n",
        "def dummy_tqdm(iterable=None, *args, **kwargs):\n",
        "    return iterable if iterable is not None else dummy_tqdm\n",
        "\n",
        "# Add all necessary attributes and methods\n",
        "dummy_tqdm.format_interval = lambda x: f\"{x:.1f}s\"\n",
        "dummy_tqdm.format_meter = lambda *args, **kwargs: \"\"\n",
        "dummy_tqdm.format_num = lambda x: str(x)\n",
        "dummy_tqdm.status_printer = lambda *args, **kwargs: lambda x: None\n",
        "dummy_tqdm.get_lock = lambda: None\n",
        "dummy_tqdm.set_lock = lambda x: None\n",
        "dummy_tqdm.display = lambda *args, **kwargs: None\n",
        "dummy_tqdm.clear = lambda *args, **kwargs: None\n",
        "dummy_tqdm.close = lambda *args, **kwargs: None\n",
        "dummy_tqdm.update = lambda *args, **kwargs: None\n",
        "dummy_tqdm.refresh = lambda *args, **kwargs: None\n",
        "dummy_tqdm.disable = True\n",
        "dummy_tqdm.monitor_interval = 0\n",
        "dummy_tqdm.monitor = None\n",
        "dummy_tqdm.pos = 0\n",
        "dummy_tqdm.__iter__ = lambda self: iter([])\n",
        "dummy_tqdm.__next__ = lambda self: next(iter([]))\n",
        "\n",
        "# Replace all tqdm variants\n",
        "tqdm.tqdm = dummy_tqdm\n",
        "tqdm.std.tqdm = dummy_tqdm\n",
        "tqdm.notebook.tqdm = dummy_tqdm\n",
        "tqdm.auto.tqdm = dummy_tqdm\n",
        "tqdm.gui.tqdm = dummy_tqdm\n",
        "tqdm.cli.tqdm = dummy_tqdm\n",
        "tqdm.__call__ = dummy_tqdm\n",
        "\n",
        "# --- Patch missing tqdm.format_sizeof --- #\n",
        "def _dummy_format_sizeof(num_bytes, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    A fake tqdm.format_sizeof.\n",
        "    Just returns a simple string of the byte count, enough to trick transformers,\n",
        "    without affecting your complete disabling of tqdm.\n",
        "    \"\"\"\n",
        "    return f\"{num_bytes}\"\n",
        "\n",
        "# If you already have a dummy_tqdm object:\n",
        "try:\n",
        "    dummy_tqdm.format_sizeof = _dummy_format_sizeof\n",
        "except NameError:\n",
        "    pass  # Skip if dummy_tqdm does not exist\n",
        "\n",
        "# Also patch the real tqdm module (or the one you replaced)\n",
        "import sys\n",
        "if 'tqdm' in sys.modules:\n",
        "    setattr(sys.modules['tqdm'], 'format_sizeof', _dummy_format_sizeof)\n",
        "\n",
        "# Set the list of frequency ratio thresholds to test\n",
        "FREQ_THRESHOLDS = [1.2, 1.4, 1.5, 1.6, 1.8, 2.0]\n",
        "\n",
        "# 1. Set up the environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3. Load the model - changed to Open LLaMA 13B\n",
        "model_name = \"openlm-research/open_llama_13b\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,   # <- 16-bit BF16\n",
        "    device_map=\"auto\",            # <- Automatically split layers across GPU/CPU\n",
        "    low_cpu_mem_usage=True        # <- Reduce peak CPU RAM usage, optional\n",
        ")\n",
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # If needed\n",
        "\n",
        "# 4. Set up hook functions to collect neuron activations\n",
        "activations = {}\n",
        "activated_neurons = {}\n",
        "ACTIVATION_THRESHOLD = 2\n",
        "\n",
        "def get_ffn_activation(name):\n",
        "    \"\"\"Hook function to capture FFN activations\"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # For the output of the SiLU activation function\n",
        "        if output.dim() >= 2:\n",
        "            # If it's a 3D tensor [batch_size, seq_len, hidden_dim]\n",
        "            if output.dim() == 3:\n",
        "                # Get only the indices after masking\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "            # If it's a 2D tensor [batch_size, hidden_dim]\n",
        "            elif output.dim() == 2:\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "    return hook\n",
        "\n",
        "# 5. Register hooks for the model\n",
        "hooks = []\n",
        "\n",
        "# Register hooks for the SiLU activation function in LlamaMLP\n",
        "# According to the model structure, the activation function is located at model.layers[i].mlp.act_fn\n",
        "for i, layer in enumerate(model.model.layers):\n",
        "    # Register MLP activation function hook - directly using the act_fn attribute\n",
        "    hook = layer.mlp.act_fn.register_forward_hook(get_ffn_activation(f'layer_{i}_mlp_act'))\n",
        "    hooks.append(hook)\n",
        "\n",
        "print(f\"Registered hooks for the SiLU activation function in the {len(model.model.layers)}-layer model\")\n",
        "\n",
        "\n",
        "# 7. Collect sample activation data\n",
        "def process_sample(sample, sample_id):\n",
        "    \"\"\"Process a single sample and collect only the index information of activated neurons\"\"\"\n",
        "    # Clear previous activations\n",
        "    activations.clear()\n",
        "    activated_neurons.clear()\n",
        "\n",
        "    # Prepare input for the sample\n",
        "    encodings = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "\n",
        "    # Run the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "\n",
        "    # Collect only the indices of activated neurons, do not store activation values\n",
        "    sample_neural_signature = {}\n",
        "    for key, value in activated_neurons.items():\n",
        "        try:\n",
        "            if len(value.shape) >= 2:\n",
        "                mask = value.squeeze(0).numpy()\n",
        "\n",
        "                # Only collect which neurons are activated in each layer, without recording position or activation value\n",
        "                if len(mask.shape) == 2:  # [seq_len, hidden_dim]\n",
        "                    # Merge the activation status across all positions\n",
        "                    # Any neuron activated at any position is considered activated\n",
        "                    active_neurons = set()\n",
        "                    for pos in range(mask.shape[0]):\n",
        "                        active_indices = np.where(mask[pos])[0]\n",
        "                        active_neurons.update(active_indices)\n",
        "\n",
        "                    if active_neurons:\n",
        "                        sample_neural_signature[key] = list(active_neurons)\n",
        "                elif len(mask.shape) == 1:  # [hidden_dim]\n",
        "                    active_indices = np.where(mask)[0]\n",
        "                    if len(active_indices) > 0:\n",
        "                        sample_neural_signature[key] = active_indices.tolist()\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    # Return sample information, containing only the label and neural signature\n",
        "    return {\n",
        "        'sample_id': sample_id,\n",
        "        'label': sample['label'],\n",
        "        'neural_signature': sample_neural_signature\n",
        "    }\n",
        "\n",
        "def collect_activations(samples, batch_size=10):\n",
        "    \"\"\"Collect activation data for samples in batches\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Process samples in batches to save memory\n",
        "    for i in range(0, len(samples), batch_size):\n",
        "        batch = samples[i:i+batch_size]\n",
        "        batch_results = []\n",
        "\n",
        "        for j, sample in enumerate(batch):\n",
        "            try:\n",
        "                result = process_sample(sample, i + j)\n",
        "                batch_results.append(result)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "        # Clean up memory\n",
        "        if i + batch_size < len(samples):  # Not the last batch\n",
        "            del batch_results\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# 8. Analyze neuron activation patterns - modified to accept frequency threshold parameter\n",
        "def analyze_neuron_activation_patterns(member_neurons, nonmember_neurons, freq_threshold=1.5):\n",
        "    \"\"\"Analyze the activation patterns of neurons in member and non-member samples, using a configurable frequency ratio threshold\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Get all layer names\n",
        "    layer_names = set()\n",
        "    for sample in member_neurons + nonmember_neurons:\n",
        "        layer_names.update(sample['neural_signature'].keys())  # Modified here\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in layer_names:\n",
        "        # Count activated neurons in member and non-member samples\n",
        "        member_neuron_counts = Counter()\n",
        "        nonmember_neuron_counts = Counter()\n",
        "\n",
        "        # Activated neurons in member samples\n",
        "        for sample in member_neurons:\n",
        "            if layer_name in sample['neural_signature']:  # Modified here\n",
        "                member_neuron_counts.update(sample['neural_signature'][layer_name])  # Modified here\n",
        "\n",
        "        # Activated neurons in non-member samples\n",
        "        for sample in nonmember_neurons:\n",
        "            if layer_name in sample['neural_signature']:  # Modified here\n",
        "                nonmember_neuron_counts.update(sample['neural_signature'][layer_name])  # Modified here\n",
        "\n",
        "        # Calculate the activation frequency for each neuron\n",
        "        member_freq = {n: count / len(member_neurons) for n, count in member_neuron_counts.items()}\n",
        "        nonmember_freq = {n: count / len(nonmember_neurons) for n, count in nonmember_neuron_counts.items()}\n",
        "\n",
        "        # Identify neurons predominantly activated in member samples, using the passed frequency threshold\n",
        "        member_dominant = {}\n",
        "        for neuron, freq in member_freq.items():\n",
        "            if neuron not in nonmember_freq or freq > nonmember_freq[neuron] * freq_threshold:\n",
        "                member_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons predominantly activated in non-member samples, using the passed frequency threshold\n",
        "        nonmember_dominant = {}\n",
        "        for neuron, freq in nonmember_freq.items():\n",
        "            if neuron not in member_freq or freq > member_freq[neuron] * freq_threshold:\n",
        "                nonmember_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons frequently activated in both types of samples\n",
        "        common_neurons = {}\n",
        "        for neuron in set(member_freq.keys()) & set(nonmember_freq.keys()):\n",
        "            if neuron not in member_dominant and neuron not in nonmember_dominant:\n",
        "                common_neurons[neuron] = (member_freq[neuron], nonmember_freq[neuron])\n",
        "\n",
        "        # Save the results\n",
        "        results[layer_name] = {\n",
        "            'member_dominant': member_dominant,\n",
        "            'nonmember_dominant': nonmember_dominant,\n",
        "            'common_neurons': common_neurons,\n",
        "            'member_counts': member_neuron_counts,\n",
        "            'nonmember_counts': nonmember_neuron_counts,\n",
        "            'member_freq': member_freq,\n",
        "            'nonmember_freq': nonmember_freq\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# 9. Build reference patterns - modified to accept frequency threshold parameter\n",
        "def build_reference_patterns(train_samples, freq_threshold=1.5, validation=False):\n",
        "    \"\"\"Build reference activation patterns using training samples, with a configurable frequency ratio threshold\"\"\"\n",
        "    # Separate member and non-member samples\n",
        "    member_samples = [s for s in train_samples if s['label'] == 1]\n",
        "    nonmember_samples = [s for s in train_samples if s['label'] == 0]\n",
        "\n",
        "    print(f\"Building reference patterns: {len(member_samples)} member samples, {len(nonmember_samples)} non-member samples\")\n",
        "\n",
        "    # Analyze activation pattern differences, passing the frequency threshold\n",
        "    reference_patterns = analyze_neuron_activation_patterns(member_samples, nonmember_samples, freq_threshold)\n",
        "\n",
        "    # If in validation phase, output reference pattern statistics\n",
        "    if validation:\n",
        "        for layer_name, data in reference_patterns.items():\n",
        "            print(f\"\\n{layer_name} Activation Pattern Analysis:\")\n",
        "            print(f\"  Neurons predominantly activated in member samples: {len(data['member_dominant'])}\")\n",
        "            print(f\"  Neurons predominantly activated in non-member samples: {len(data['nonmember_dominant'])}\")\n",
        "            print(f\"  Neurons activated in both types of samples: {len(data['common_neurons'])}\")\n",
        "\n",
        "    return reference_patterns\n",
        "\n",
        "# 10. Calculate discrimination score for each layer\n",
        "def calculate_layer_discrimination_scores(reference_patterns):\n",
        "    \"\"\"Calculate the discriminative power score for each layer\"\"\"\n",
        "    layer_scores = {}\n",
        "\n",
        "    # Calculate the discriminative power score for each layer\n",
        "    for layer_name, data in reference_patterns.items():\n",
        "        # Simple discrimination score: number of member-dominant neurons minus number of non-member-dominant neurons\n",
        "        # If the score is greater than 0, it indicates the layer is more inclined to identify member samples\n",
        "        member_dominant_count = len(data['member_dominant'])\n",
        "        nonmember_dominant_count = len(data['nonmember_dominant'])\n",
        "\n",
        "        discrimination_score = member_dominant_count - nonmember_dominant_count\n",
        "        layer_scores[layer_name] = discrimination_score\n",
        "\n",
        "    # Print layer discrimination scores\n",
        "    print(\"\\nLayer Discrimination Scores:\")\n",
        "    sorted_scores = sorted(layer_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for layer_name, score in sorted_scores:\n",
        "        print(f\"  {layer_name}: {score}\")\n",
        "\n",
        "    return layer_scores\n",
        "\n",
        "# 11. Select the most discriminative layers\n",
        "def select_discriminative_layers(layer_scores, top_n=10):\n",
        "    \"\"\"Select the most discriminative layers\"\"\"\n",
        "    # Sort by discrimination score\n",
        "    sorted_scores = sorted(layer_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    # Select the top N layers\n",
        "    selected_layers = [layer for layer, score in sorted_scores[:top_n]]\n",
        "\n",
        "    print(\"Selected most discriminative layers:\")\n",
        "    for layer_name, score in sorted_scores[:top_n]:\n",
        "        print(f\"  {layer_name}: Score {score}\")\n",
        "\n",
        "    return selected_layers\n",
        "\n",
        "# 12. Membership prediction based on relative ratio\n",
        "def predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers, threshold=1.0):\n",
        "    \"\"\"Predict whether a single sample is a member using the relative ratio method\"\"\"\n",
        "    if not sample['neural_signature']:  # Modified here\n",
        "        return 0, 0, 0  # If there is no activation data, default to non-member\n",
        "\n",
        "    # Initialize relative ratios\n",
        "    layers_counted = 0\n",
        "    total_member_ratio = 0\n",
        "    total_nonmember_ratio = 0\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in discriminative_layers:\n",
        "        if layer_name not in sample['neural_signature'] or layer_name not in reference_patterns:  # Modified here\n",
        "            continue\n",
        "\n",
        "        # Get the reference pattern for this layer\n",
        "        layer_data = reference_patterns[layer_name]\n",
        "\n",
        "        # Get all neurons activated by this sample in this layer\n",
        "        sample_neurons = set(sample['neural_signature'][layer_name])  # Modified here\n",
        "\n",
        "        if not sample_neurons:\n",
        "            continue\n",
        "\n",
        "        # Calculate the relative overlap with member-dominant neurons\n",
        "        member_dominant_set = set(layer_data['member_dominant'].keys())\n",
        "        member_overlap = len(sample_neurons.intersection(member_dominant_set))\n",
        "\n",
        "        # Calculate the relative overlap with non-member-dominant neurons\n",
        "        nonmember_dominant_set = set(layer_data['nonmember_dominant'].keys())\n",
        "        nonmember_overlap = len(sample_neurons.intersection(nonmember_dominant_set))\n",
        "\n",
        "        # Calculate relative ratios\n",
        "        member_ratio = member_overlap / len(member_dominant_set) if len(member_dominant_set) > 0 else 0\n",
        "        nonmember_ratio = nonmember_overlap / len(nonmember_dominant_set) if len(nonmember_dominant_set) > 0 else 0\n",
        "\n",
        "        # Accumulate relative ratios\n",
        "        total_member_ratio += member_ratio\n",
        "        total_nonmember_ratio += nonmember_ratio\n",
        "        layers_counted += 1\n",
        "\n",
        "    # If there are no valid layers, default to non-member\n",
        "    if layers_counted == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    # Calculate average relative ratios\n",
        "    avg_member_ratio = total_member_ratio / layers_counted\n",
        "    avg_nonmember_ratio = total_nonmember_ratio / layers_counted\n",
        "\n",
        "    # Calculate the ratio of relative ratios\n",
        "    if avg_nonmember_ratio == 0:\n",
        "        ratio = float('inf')  # Avoid division by zero\n",
        "    else:\n",
        "        ratio = avg_member_ratio / avg_nonmember_ratio\n",
        "\n",
        "    # Return the prediction and related ratios\n",
        "    return 1 if ratio >= threshold else 0, avg_member_ratio, avg_nonmember_ratio\n",
        "\n",
        "# 14. Find the best threshold on the validation set\n",
        "def find_best_relative_ratio_threshold(val_samples, reference_patterns, discriminative_layers):\n",
        "    \"\"\"Find the best threshold on the validation set using the relative ratio method\"\"\"\n",
        "    # Calculate relative ratios for all validation samples\n",
        "    ratios = []\n",
        "    labels = []\n",
        "\n",
        "    for sample in val_samples:\n",
        "        pred, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "            sample, reference_patterns, discriminative_layers, threshold=1.0\n",
        "        )\n",
        "\n",
        "        # Calculate the ratio of relative ratios\n",
        "        ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "        ratios.append(ratio)\n",
        "        labels.append(sample['label'])\n",
        "\n",
        "    # Filter out infinite values\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for r, l in zip(ratios, labels):\n",
        "        if r != float('inf') and not np.isnan(r):\n",
        "            filtered_ratios.append(r)\n",
        "            filtered_labels.append(l)\n",
        "\n",
        "    # Create candidate thresholds\n",
        "    min_ratio = min(filtered_ratios)\n",
        "    max_ratio = max(filtered_ratios)\n",
        "\n",
        "    # Generate uniformly distributed thresholds\n",
        "    candidate_thresholds = list(np.linspace(min_ratio, max_ratio, 100))\n",
        "    # Add some important threshold points\n",
        "    candidate_thresholds += [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n",
        "    candidate_thresholds = sorted(set(candidate_thresholds))\n",
        "\n",
        "    # Find the best threshold\n",
        "    best_threshold = 1.0\n",
        "    best_accuracy = 0\n",
        "    best_metrics = None\n",
        "\n",
        "    results = []\n",
        "    for threshold in candidate_thresholds:\n",
        "        # Make predictions using the current threshold\n",
        "        predictions = [1 if r >= threshold else 0 for r in ratios]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "\n",
        "        # Record the results\n",
        "        results.append({\n",
        "            'threshold': threshold,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        # Update the best threshold\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "    print(f\"Best relative ratio threshold: {best_threshold:.4f}\")\n",
        "    print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"Precision: {best_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {best_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {best_metrics['f1']:.4f}\")\n",
        "\n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Simplified function to test different frequency thresholds, returns only AUC values\n",
        "def run_frequency_threshold_sensitivity_test():\n",
        "    \"\"\"Run a sensitivity analysis for different frequency ratio thresholds, returning only AUC values\"\"\"\n",
        "    try:\n",
        "        from random import seed\n",
        "        seed(42)  # Ensure reproducibility\n",
        "\n",
        "        # Frequency thresholds to test\n",
        "        freq_thresholds = FREQ_THRESHOLDS\n",
        "        print(f\"Starting frequency ratio threshold sensitivity analysis...\")\n",
        "        print(f\"Frequency ratio thresholds to be tested: {freq_thresholds}\")\n",
        "        print(f\"Fixed activation threshold: {ACTIVATION_THRESHOLD}\")\n",
        "\n",
        "        # Store AUC results for different thresholds\n",
        "        threshold_aucs = {}\n",
        "\n",
        "        # Get member and non-member samples from the development set\n",
        "        dev_member = [item for item in ds_dev if item['label'] == 1]\n",
        "        dev_nonmember = [item for item in ds_dev if item['label'] == 0]\n",
        "\n",
        "        # Split the development set into 80/20 for training and validation\n",
        "        train_size_member = int(len(dev_member) * 0.8)\n",
        "        train_size_nonmember = int(len(dev_nonmember) * 0.8)\n",
        "\n",
        "        train_member = dev_member[:train_size_member]\n",
        "        train_nonmember = dev_nonmember[:train_size_nonmember]\n",
        "\n",
        "        val_member = dev_member[train_size_member:]\n",
        "        val_nonmember = dev_nonmember[train_size_nonmember:]\n",
        "\n",
        "        print(f\"Training set: {len(train_member)} member samples, {len(train_nonmember)} non-member samples\")\n",
        "        print(f\"Validation set: {len(val_member)} member samples, {len(val_nonmember)} non-member samples\")\n",
        "\n",
        "        # Collect activation data for training and validation samples\n",
        "        print(\"\\nCollecting training sample activations...\")\n",
        "        train_member_acts = collect_activations(train_member)\n",
        "        train_nonmember_acts = collect_activations(train_nonmember)\n",
        "\n",
        "        print(\"\\nCollecting validation sample activations...\")\n",
        "        val_member_acts = collect_activations(val_member)\n",
        "        val_nonmember_acts = collect_activations(val_nonmember)\n",
        "\n",
        "        val_samples = val_member_acts + val_nonmember_acts\n",
        "\n",
        "        # Test for each frequency ratio threshold\n",
        "        for freq_threshold in freq_thresholds:\n",
        "            print(f\"\\n===== Testing frequency ratio threshold: {freq_threshold} =====\")\n",
        "\n",
        "            # Build reference activation patterns using the current frequency ratio threshold\n",
        "            print(f\"\\nBuilding reference activation patterns (frequency threshold={freq_threshold})...\")\n",
        "            reference_patterns = build_reference_patterns(\n",
        "                train_member_acts + train_nonmember_acts,\n",
        "                freq_threshold=freq_threshold,\n",
        "                validation=True\n",
        "            )\n",
        "\n",
        "            # Calculate layer discrimination scores\n",
        "            print(\"\\nCalculating layer discrimination scores...\")\n",
        "            layer_scores = calculate_layer_discrimination_scores(reference_patterns)\n",
        "\n",
        "            # Select the most discriminative layers\n",
        "            print(\"\\nSelecting the most discriminative layers...\")\n",
        "            discriminative_layers = select_discriminative_layers(layer_scores, top_n=10)\n",
        "\n",
        "            # Find the best relative ratio threshold on the validation set\n",
        "            print(\"\\nFinding the best threshold on the validation set using the relative ratio method...\")\n",
        "            best_threshold, val_metrics = find_best_relative_ratio_threshold(\n",
        "                val_samples,\n",
        "                reference_patterns,\n",
        "                discriminative_layers\n",
        "            )\n",
        "\n",
        "            # Calculate relative ratios for all validation samples\n",
        "            ratios = []\n",
        "            labels = []\n",
        "\n",
        "            for sample in val_samples:\n",
        "                pred, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "                    sample, reference_patterns, discriminative_layers, threshold=best_threshold\n",
        "                )\n",
        "\n",
        "                # Calculate the ratio of relative ratios\n",
        "                ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "                ratios.append(ratio)\n",
        "                labels.append(sample['label'])\n",
        "\n",
        "            # Filter out infinite values\n",
        "            filtered_ratios = []\n",
        "            filtered_labels = []\n",
        "            for r, l in zip(ratios, labels):\n",
        "                if r != float('inf') and not np.isnan(r):\n",
        "                    filtered_ratios.append(r)\n",
        "                    filtered_labels.append(l)\n",
        "\n",
        "            # Calculate AUC\n",
        "            fpr, tpr, _ = roc_curve(filtered_labels, filtered_ratios)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            # Record the AUC result for the current frequency threshold\n",
        "            threshold_aucs[freq_threshold] = roc_auc\n",
        "\n",
        "            print(f\"AUC for frequency threshold {freq_threshold}: {roc_auc:.4f}\")\n",
        "\n",
        "        # Find the best frequency threshold\n",
        "        best_freq_threshold = max(freq_thresholds, key=lambda t: threshold_aucs[t])\n",
        "\n",
        "        print(\"\\n===== Frequency Ratio Threshold Sensitivity Analysis Results =====\")\n",
        "        print(f\"Best frequency ratio threshold: {best_freq_threshold}\")\n",
        "        print(f\"Best AUC: {threshold_aucs[best_freq_threshold]:.4f}\")\n",
        "\n",
        "        # Data in table format\n",
        "        print(\"\\nAUC Value Table:\")\n",
        "        print(\"Freq. Threshold | AUC\")\n",
        "        print(\"----------------|-----\")\n",
        "        for freq in freq_thresholds:\n",
        "            print(f\"{freq:.1f}             | {threshold_aucs[freq]:.4f}\")\n",
        "\n",
        "        return threshold_aucs, best_freq_threshold\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {str(e)}\")\n",
        "        # Ensure hooks are removed\n",
        "        for hook in hooks:\n",
        "            try:\n",
        "                hook.remove()\n",
        "            except:\n",
        "                pass\n",
        "        raise e\n",
        "\n",
        "\n",
        "# Run Membership Inference Attack\n",
        "if __name__ == \"__main__\":\n",
        "    # Run frequency threshold sensitivity test\n",
        "    threshold_aucs, best_freq_threshold = run_frequency_threshold_sensitivity_test()\n",
        "    print(f\"\\nFrequency threshold sensitivity analysis complete! The best frequency ratio threshold is: {best_freq_threshold}\")\n",
        "\n",
        "    # Clean up resources\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    print(\"Program execution finished!\")"
      ],
      "metadata": {
        "id": "33G6bZ5SKgvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CCNewsPDD"
      ],
      "metadata": {
        "id": "fcQUE2R42fmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pythia-2.8b"
      ],
      "metadata": {
        "id": "H9G1ZT1dLCEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_fscore_support\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable tqdm - a safer way\n",
        "import sys\n",
        "import importlib\n",
        "# Completely disable tqdm\n",
        "import tqdm\n",
        "\n",
        "# Define a function that does nothing and returns its input\n",
        "def dummy_tqdm(iterable=None, *args, **kwargs):\n",
        "    return iterable if iterable is not None else dummy_tqdm\n",
        "\n",
        "# Add all necessary attributes and methods\n",
        "dummy_tqdm.format_interval = lambda x: f\"{x:.1f}s\"\n",
        "dummy_tqdm.format_meter = lambda *args, **kwargs: \"\"\n",
        "dummy_tqdm.format_num = lambda x: str(x)\n",
        "dummy_tqdm.status_printer = lambda *args, **kwargs: lambda x: None\n",
        "dummy_tqdm.get_lock = lambda: None\n",
        "dummy_tqdm.set_lock = lambda x: None\n",
        "dummy_tqdm.display = lambda *args, **kwargs: None\n",
        "dummy_tqdm.clear = lambda *args, **kwargs: None\n",
        "dummy_tqdm.close = lambda *args, **kwargs: None\n",
        "dummy_tqdm.update = lambda *args, **kwargs: None\n",
        "dummy_tqdm.refresh = lambda *args, **kwargs: None\n",
        "dummy_tqdm.disable = True\n",
        "dummy_tqdm.monitor_interval = 0\n",
        "dummy_tqdm.monitor = None\n",
        "dummy_tqdm.pos = 0\n",
        "dummy_tqdm.__iter__ = lambda self: iter([])\n",
        "dummy_tqdm.__next__ = lambda self: next(iter([]))\n",
        "\n",
        "# Replace all tqdm variants\n",
        "tqdm.tqdm = dummy_tqdm\n",
        "tqdm.std.tqdm = dummy_tqdm\n",
        "tqdm.notebook.tqdm = dummy_tqdm\n",
        "tqdm.auto.tqdm = dummy_tqdm\n",
        "tqdm.gui.tqdm = dummy_tqdm\n",
        "tqdm.cli.tqdm = dummy_tqdm\n",
        "tqdm.__call__ = dummy_tqdm\n",
        "\n",
        "# --- Patch missing tqdm.format_sizeof --- #\n",
        "def _dummy_format_sizeof(num_bytes, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    A fake tqdm.format_sizeof.\n",
        "    Just returns a simple string of the byte count, enough to trick transformers,\n",
        "    without affecting your complete disabling of tqdm.\n",
        "    \"\"\"\n",
        "    return f\"{num_bytes}\"\n",
        "\n",
        "# If you already have a dummy_tqdm object:\n",
        "try:\n",
        "    dummy_tqdm.format_sizeof = _dummy_format_sizeof\n",
        "except NameError:\n",
        "    pass  # Skip if dummy_tqdm does not exist\n",
        "\n",
        "# Also patch the real tqdm module (or the one you replaced)\n",
        "import sys\n",
        "if 'tqdm' in sys.modules:\n",
        "    setattr(sys.modules['tqdm'], 'format_sizeof', _dummy_format_sizeof)\n",
        "\n",
        "# 1. Set up the environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Core Change 1: Switch model to pythia-2.8b ---\n",
        "model_name = \"EleutherAI/pythia-2.8b\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Specify 16-bit precision\n",
        "    device_map=\"auto\"           # Automatically manage model distribution on GPU/CPU\n",
        ")\n",
        "# --- End of change ---\n",
        "\n",
        "# Load the corresponding tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# If the tokenizer does not have a padding token, set one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded: {model_name}, using {model.dtype} precision\")\n",
        "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Display model memory usage\n",
        "def get_model_size(model):\n",
        "    \"\"\"Calculate model size (GB)\"\"\"\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**3\n",
        "    return model_size\n",
        "\n",
        "model_size_gb = get_model_size(model)\n",
        "print(f\"Model size: {model_size_gb:.2f} GB\")\n",
        "\n",
        "# 4. Set up hook functions to collect neuron activations\n",
        "activations = {}\n",
        "activated_neurons = {}\n",
        "ACTIVATION_THRESHOLD = 0  # Set activation threshold\n",
        "\n",
        "def get_ffn_activation(name):\n",
        "    \"\"\"Hook function to capture FFN activations\"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # Save activation values\n",
        "        activations[name] = output[0].detach().cpu() # Pythia FFN output is a tuple\n",
        "\n",
        "        # Identify activated neurons (neurons exceeding the threshold)\n",
        "        act_output = output[0]\n",
        "        if act_output.dim() >= 2:\n",
        "            if act_output.dim() == 3: # [batch_size, seq_len, hidden_dim]\n",
        "                activation_mask = (act_output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "            elif act_output.dim() == 2: # [batch_size, hidden_dim]\n",
        "                activation_mask = (act_output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "    return hook\n",
        "\n",
        "# 5. Register hooks for the model\n",
        "hooks = []\n",
        "\n",
        "# --- Core Change 2: Adapt to Pythia/GPT-NeoX model structure ---\n",
        "# Analyze the structure of the first layer to understand the location of the activation function\n",
        "sample_layer = model.gpt_neox.layers[0]\n",
        "print(\"Layer Structure Analysis (Pythia/GPT-NeoX):\")\n",
        "for name, module in sample_layer.named_modules():\n",
        "    print(f\"  - {name}: {type(module).__name__}\")\n",
        "\n",
        "# Register hooks for FFN layers - targeting the Pythia/GPT-NeoX model architecture\n",
        "for i, layer in enumerate(model.gpt_neox.layers):\n",
        "    # The activation function in Pythia/GPT-NeoX is usually at layer.mlp.act\n",
        "    try:\n",
        "        module_to_hook = layer.mlp.act\n",
        "        print(f\"Registering hook for mlp.act module ({type(module_to_hook).__name__}) in layer {i}\")\n",
        "        hook = module_to_hook.register_forward_hook(get_ffn_activation(f'layer_{i}_mlp_act'))\n",
        "        hooks.append(hook)\n",
        "    except AttributeError:\n",
        "        print(f\"Warning: mlp.act module not found in layer {i}\")\n",
        "# --- End of change ---\n",
        "\n",
        "# 7. Collect sample activation data\n",
        "def process_sample(sample, sample_id):\n",
        "    \"\"\"Process a single sample and collect activations\"\"\"\n",
        "    # Clear previous activations\n",
        "    activations.clear()\n",
        "    activated_neurons.clear()\n",
        "\n",
        "    # Prepare input for the sample\n",
        "    encodings = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    encodings = {k: v.to(model.device) for k, v in encodings.items()}\n",
        "\n",
        "    # Run the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "\n",
        "    # Collect activated neurons\n",
        "    sample_activated_neurons = {}\n",
        "    for key, value in activated_neurons.items():\n",
        "        try:\n",
        "            if len(value.shape) >= 2:\n",
        "                # Remove batch dimension\n",
        "                mask = value.squeeze(0).numpy()\n",
        "\n",
        "                # For each position, record the indices of activated neurons\n",
        "                if len(mask.shape) == 2:  # [seq_len, hidden_dim]\n",
        "                    position_neurons = {}\n",
        "                    for pos in range(mask.shape[0]):\n",
        "                        active_indices = np.where(mask[pos])[0]\n",
        "                        if len(active_indices) > 0:\n",
        "                            position_neurons[pos] = active_indices.tolist()\n",
        "                    if position_neurons:\n",
        "                        sample_activated_neurons[key] = position_neurons\n",
        "                elif len(mask.shape) == 1:  # [hidden_dim]\n",
        "                    active_indices = np.where(mask)[0]\n",
        "                    if len(active_indices) > 0:\n",
        "                        sample_activated_neurons[key] = {0: active_indices.tolist()}\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    # Return sample information\n",
        "    return {\n",
        "        'sample_id': sample_id,\n",
        "        'text': sample['text'],\n",
        "        'label': sample['label'],\n",
        "        'activated_neurons': sample_activated_neurons,\n",
        "        'input_ids': encodings['input_ids'][0].cpu().numpy(),\n",
        "    }\n",
        "\n",
        "def collect_activations(samples, batch_size=10):\n",
        "    \"\"\"Collect activation data for samples in batches\"\"\"\n",
        "    results = []\n",
        "    # Use a simple for loop instead of tqdm\n",
        "    for i in range(0, len(samples), batch_size):\n",
        "        batch = samples[i:i+batch_size]\n",
        "        batch_results = []\n",
        "        for j, sample in enumerate(batch):\n",
        "            try:\n",
        "                result = process_sample(sample, i + j)\n",
        "                batch_results.append(result)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "        results.extend(batch_results)\n",
        "        if i + batch_size < len(samples):\n",
        "            del batch_results\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "    return results\n",
        "\n",
        "# 8. Analyze neuron activation patterns\n",
        "def analyze_neuron_activation_patterns(member_neurons, nonmember_neurons):\n",
        "    \"\"\"Analyze the activation patterns of neurons in member and non-member samples\"\"\"\n",
        "    results = {}\n",
        "    layer_names = set()\n",
        "    for sample in member_neurons + nonmember_neurons:\n",
        "        layer_names.update(sample['activated_neurons'].keys())\n",
        "\n",
        "    for layer_name in layer_names:\n",
        "        member_neuron_counts = Counter()\n",
        "        nonmember_neuron_counts = Counter()\n",
        "        for sample in member_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    member_neuron_counts.update(pos_neurons)\n",
        "        for sample in nonmember_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    nonmember_neuron_counts.update(pos_neurons)\n",
        "\n",
        "        member_freq = {n: count / len(member_neurons) for n, count in member_neuron_counts.items()} if member_neurons else {}\n",
        "        nonmember_freq = {n: count / len(nonmember_neurons) for n, count in nonmember_neuron_counts.items()} if nonmember_neurons else {}\n",
        "\n",
        "        member_dominant = {n: f for n, f in member_freq.items() if n not in nonmember_freq or f > nonmember_freq.get(n, 0) * 1.5}\n",
        "        nonmember_dominant = {n: f for n, f in nonmember_freq.items() if n not in member_freq or f > member_freq.get(n, 0) * 1.5}\n",
        "        common_neurons = {n: (member_freq[n], nonmember_freq[n]) for n in set(member_freq.keys()) & set(nonmember_freq.keys()) if n not in member_dominant and n not in nonmember_dominant}\n",
        "\n",
        "        results[layer_name] = {\n",
        "            'member_dominant': member_dominant, 'nonmember_dominant': nonmember_dominant,\n",
        "            'common_neurons': common_neurons, 'member_counts': member_neuron_counts,\n",
        "            'nonmember_counts': nonmember_neuron_counts, 'member_freq': member_freq,\n",
        "            'nonmember_freq': nonmember_freq\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# 9. Build reference patterns\n",
        "def build_reference_patterns(train_samples):\n",
        "    \"\"\"Build reference activation patterns using training samples\"\"\"\n",
        "    member_samples = [s for s in train_samples if s['label'] == 1]\n",
        "    nonmember_samples = [s for s in train_samples if s['label'] == 0]\n",
        "    return analyze_neuron_activation_patterns(member_samples, nonmember_samples)\n",
        "\n",
        "# 10. Calculate discrimination score for each layer\n",
        "def calculate_layer_discrimination_scores(reference_patterns):\n",
        "    \"\"\"Calculate the discriminative power score for each layer\"\"\"\n",
        "    return {\n",
        "        layer_name: len(data['member_dominant']) - len(data['nonmember_dominant'])\n",
        "        for layer_name, data in reference_patterns.items()\n",
        "    }\n",
        "\n",
        "# 11. Select the most discriminative layers\n",
        "def select_discriminative_layers(layer_scores, top_n=10):\n",
        "    \"\"\"Select the most discriminative layers\"\"\"\n",
        "    sorted_scores = sorted(layer_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "    return [layer for layer, score in sorted_scores[:top_n]]\n",
        "\n",
        "# 12. Membership prediction based on relative ratio\n",
        "def predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers, threshold=1.0):\n",
        "    \"\"\"Predict whether a single sample is a member using the relative ratio method\"\"\"\n",
        "    if not sample['activated_neurons']:\n",
        "        return 0, 0, 0\n",
        "    layers_counted = 0\n",
        "    total_member_ratio = 0\n",
        "    total_nonmember_ratio = 0\n",
        "    for layer_name in discriminative_layers:\n",
        "        if layer_name not in sample['activated_neurons'] or layer_name not in reference_patterns:\n",
        "            continue\n",
        "        layer_data = reference_patterns[layer_name]\n",
        "        sample_neurons = set()\n",
        "        for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "            sample_neurons.update(pos_neurons)\n",
        "        if not sample_neurons:\n",
        "            continue\n",
        "        member_dominant_set = set(layer_data['member_dominant'].keys())\n",
        "        nonmember_dominant_set = set(layer_data['nonmember_dominant'].keys())\n",
        "        member_overlap = len(sample_neurons.intersection(member_dominant_set))\n",
        "        nonmember_overlap = len(sample_neurons.intersection(nonmember_dominant_set))\n",
        "        member_ratio = member_overlap / len(member_dominant_set) if len(member_dominant_set) > 0 else 0\n",
        "        nonmember_ratio = nonmember_overlap / len(nonmember_dominant_set) if len(nonmember_dominant_set) > 0 else 0\n",
        "        total_member_ratio += member_ratio\n",
        "        total_nonmember_ratio += nonmember_ratio\n",
        "        layers_counted += 1\n",
        "    if layers_counted == 0:\n",
        "        return 0, 0, 0\n",
        "    avg_member_ratio = total_member_ratio / layers_counted\n",
        "    avg_nonmember_ratio = total_nonmember_ratio / layers_counted\n",
        "    ratio = float('inf') if avg_nonmember_ratio == 0 else avg_member_ratio / avg_nonmember_ratio\n",
        "    return 1 if ratio >= threshold else 0, avg_member_ratio, avg_nonmember_ratio\n",
        "\n",
        "# 14. Find the best threshold on the validation set\n",
        "def find_best_relative_ratio_threshold(val_samples, reference_patterns, discriminative_layers):\n",
        "    \"\"\"Find the best threshold on the validation set using the relative ratio method\"\"\"\n",
        "    ratios, labels = [], []\n",
        "    for sample in val_samples:\n",
        "        _, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers)\n",
        "        ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "        ratios.append(ratio)\n",
        "        labels.append(sample['label'])\n",
        "\n",
        "    filtered_ratios = [r for r in ratios if r != float('inf') and not np.isnan(r)]\n",
        "    if not filtered_ratios:\n",
        "        return 1.0, {'accuracy': 0.5, 'precision': 0, 'recall': 0, 'f1': 0}\n",
        "\n",
        "    candidate_thresholds = sorted(set(list(np.linspace(min(filtered_ratios), max(filtered_ratios), 100)) + [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]))\n",
        "    best_accuracy, best_threshold, best_metrics = 0, 1.0, None\n",
        "    for threshold in candidate_thresholds:\n",
        "        predictions = [1 if r >= threshold else 0 for r in ratios]\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = threshold\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary', zero_division=0)\n",
        "            best_metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Calculate AUC and TPR@5%FPR for the test set\n",
        "def calculate_auc_and_tpr(test_samples, reference_patterns, discriminative_layers, batch_size=10):\n",
        "    \"\"\"Calculate the AUC value and TPR@5%FPR for the test set\"\"\"\n",
        "    all_ratios, all_labels = [], []\n",
        "    for i in range(0, len(test_samples), batch_size):\n",
        "        batch = test_samples[i:i+batch_size]\n",
        "        batch_acts = collect_activations(batch, batch_size=batch_size)\n",
        "        for sample in batch_acts:\n",
        "            _, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers)\n",
        "            ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "            all_ratios.append(ratio)\n",
        "            all_labels.append(sample['label'])\n",
        "        del batch_acts\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    filtered_ratios, filtered_labels = [], []\n",
        "    for ratio, label in zip(all_ratios, all_labels):\n",
        "        if ratio != float('inf') and not np.isnan(ratio):\n",
        "            filtered_ratios.append(ratio)\n",
        "            filtered_labels.append(label)\n",
        "    if len(set(filtered_labels)) < 2:\n",
        "        return 0.5, 0.0\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(filtered_ratios, filtered_labels)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    tpr_at_fpr = 0.0\n",
        "    try:\n",
        "        target_index = np.where(fpr >= 0.05)[0][0]\n",
        "        tpr_at_fpr = tpr[target_index]\n",
        "    except IndexError:\n",
        "        pass\n",
        "    return roc_auc, tpr_at_fpr\n",
        "\n",
        "# Process all thresholds for a single dataset\n",
        "def process_dataset(data_type, data_dir=\"/content/drive/MyDrive/LLM_MIA/data\", thresholds=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]):\n",
        "    \"\"\"Process all thresholds for a single dataset\"\"\"\n",
        "    global ACTIVATION_THRESHOLD\n",
        "    print(f\"\\n===== Dataset: {data_type} =====\")\n",
        "\n",
        "    # Load dev and test based on the data_type condition\n",
        "    if data_type == \"prompt\":\n",
        "        dev_path = os.path.join(data_dir, \"pile_cc_mia_dev_prompt_gpt4o_simple_v1\")\n",
        "        test_path = os.path.join(data_dir, \"pile_cc_mia_test_prompt_gpt4o_simple_v1\")\n",
        "        print(f\"Loading specific datasets for 'prompt' type:\")\n",
        "        print(f\"  - DEV: {dev_path}\")\n",
        "        print(f\"  - TEST: {test_path}\")\n",
        "        ds_dev = load_from_disk(dev_path)\n",
        "        ds_test = load_from_disk(test_path)\n",
        "    else:\n",
        "        ds_dev = load_from_disk(os.path.join(data_dir, f\"pile_cc_mia_dev_{data_type}\"))\n",
        "        ds_test = load_from_disk(os.path.join(data_dir, f\"pile_cc_mia_test_{data_type}\"))\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        ACTIVATION_THRESHOLD = threshold\n",
        "        dev_member = [item for item in ds_dev if item['label'] == 1]\n",
        "        dev_nonmember = [item for item in ds_dev if item['label'] == 0]\n",
        "        train_member, val_member = dev_member[:len(dev_member)//2], dev_member[len(dev_member)//2:]\n",
        "        train_nonmember, val_nonmember = dev_nonmember[:len(dev_nonmember)//2], dev_nonmember[len(dev_nonmember)//2:]\n",
        "\n",
        "        train_member_acts = collect_activations(train_member)\n",
        "        train_nonmember_acts = collect_activations(train_nonmember)\n",
        "        reference_patterns = build_reference_patterns(train_member_acts + train_nonmember_acts)\n",
        "\n",
        "        layer_scores = calculate_layer_discrimination_scores(reference_patterns)\n",
        "        discriminative_layers = select_discriminative_layers(layer_scores, top_n=10)\n",
        "\n",
        "        # (Optional) Find the best threshold on the validation set - this part's result is unused in your code, so it can be simplified\n",
        "        # val_member_acts = collect_activations(val_member)\n",
        "        # val_nonmember_acts = collect_activations(val_nonmember)\n",
        "        # val_samples = val_member_acts + val_nonmember_acts\n",
        "        # best_threshold, val_metrics = find_best_relative_ratio_threshold(\n",
        "        #     val_samples, reference_patterns, discriminative_layers\n",
        "        # )\n",
        "\n",
        "        test_auc, test_tpr_at_fpr = calculate_auc_and_tpr(list(ds_test), reference_patterns, discriminative_layers)\n",
        "        print(f\"Dataset: {data_type}, Activation Threshold: {threshold:.1f}, Test Set AUC = {test_auc:.4f}, TPR@5%FPR (%) = {test_tpr_at_fpr * 100:.2f}%\")\n",
        "\n",
        "        del train_member_acts, train_nonmember_acts, reference_patterns, layer_scores, discriminative_layers\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    del ds_dev, ds_test\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        activation_thresholds = [1.0]\n",
        "        datasets = [\"bt\", \"bert\", \"prompt\"]\n",
        "        for dataset in datasets:\n",
        "            process_dataset(dataset, thresholds=activation_thresholds)\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {str(e)}\")\n",
        "        for hook in hooks:\n",
        "            try: hook.remove()\n",
        "            except: pass\n",
        "        raise e"
      ],
      "metadata": {
        "id": "1ATeMx7y2h3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Opt-6.7b"
      ],
      "metadata": {
        "id": "Oa7Zdp28LDsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_fscore_support\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable tqdm - a safer way\n",
        "import sys\n",
        "import importlib\n",
        "# Completely disable tqdm\n",
        "import tqdm\n",
        "\n",
        "# Define a function that does nothing and returns its input\n",
        "def dummy_tqdm(iterable=None, *args, **kwargs):\n",
        "    return iterable if iterable is not None else dummy_tqdm\n",
        "\n",
        "# Add all necessary attributes and methods\n",
        "dummy_tqdm.format_interval = lambda x: f\"{x:.1f}s\"\n",
        "dummy_tqdm.format_meter = lambda *args, **kwargs: \"\"\n",
        "dummy_tqdm.format_num = lambda x: str(x)\n",
        "dummy_tqdm.status_printer = lambda *args, **kwargs: lambda x: None\n",
        "dummy_tqdm.get_lock = lambda: None\n",
        "dummy_tqdm.set_lock = lambda x: None\n",
        "dummy_tqdm.display = lambda *args, **kwargs: None\n",
        "dummy_tqdm.clear = lambda *args, **kwargs: None\n",
        "dummy_tqdm.close = lambda *args, **kwargs: None\n",
        "dummy_tqdm.update = lambda *args, **kwargs: None\n",
        "dummy_tqdm.refresh = lambda *args, **kwargs: None\n",
        "dummy_tqdm.disable = True\n",
        "dummy_tqdm.monitor_interval = 0\n",
        "dummy_tqdm.monitor = None\n",
        "dummy_tqdm.pos = 0\n",
        "dummy_tqdm.__iter__ = lambda self: iter([])\n",
        "dummy_tqdm.__next__ = lambda self: next(iter([]))\n",
        "\n",
        "# Replace all tqdm variants\n",
        "tqdm.tqdm = dummy_tqdm\n",
        "tqdm.std.tqdm = dummy_tqdm\n",
        "tqdm.notebook.tqdm = dummy_tqdm\n",
        "tqdm.auto.tqdm = dummy_tqdm\n",
        "tqdm.gui.tqdm = dummy_tqdm\n",
        "tqdm.cli.tqdm = dummy_tqdm\n",
        "tqdm.__call__ = dummy_tqdm\n",
        "\n",
        "# --- Patch missing tqdm.format_sizeof --- #\n",
        "def _dummy_format_sizeof(num_bytes, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    A fake tqdm.format_sizeof.\n",
        "    Just returns a simple string of the byte count, enough to trick transformers,\n",
        "    without affecting your complete disabling of tqdm.\n",
        "    \"\"\"\n",
        "    return f\"{num_bytes}\"\n",
        "\n",
        "# If you already have a dummy_tqdm object:\n",
        "try:\n",
        "    dummy_tqdm.format_sizeof = _dummy_format_sizeof\n",
        "except NameError:\n",
        "    pass  # Skip if dummy_tqdm does not exist\n",
        "\n",
        "# Also patch the real tqdm module (or the one you replaced)\n",
        "import sys\n",
        "if 'tqdm' in sys.modules:\n",
        "    setattr(sys.modules['tqdm'], 'format_sizeof', _dummy_format_sizeof)\n",
        "\n",
        "# 1. Set up the environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the OPT-6.7B model with 16-bit precision (half-precision, FP16)\n",
        "model_name = \"facebook/opt-6.7b\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Specify 16-bit precision\n",
        "    device_map=\"auto\"           # Automatically manage model distribution on GPU/CPU\n",
        ")\n",
        "\n",
        "# Load the corresponding tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# If the tokenizer does not have a padding token, set one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded, using {model.dtype} precision\")\n",
        "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Display model memory usage\n",
        "def get_model_size(model):\n",
        "    \"\"\"Calculate model size (GB)\"\"\"\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**3\n",
        "    return model_size\n",
        "\n",
        "model_size_gb = get_model_size(model)\n",
        "print(f\"Model size: {model_size_gb:.2f} GB\")\n",
        "\n",
        "# If the tokenizer does not have a padding token, set one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 4. Set up hook functions to collect neuron activations\n",
        "activations = {}\n",
        "activated_neurons = {}\n",
        "ACTIVATION_THRESHOLD = 0  # Set activation threshold\n",
        "\n",
        "def get_ffn_activation(name):\n",
        "    \"\"\"Hook function to capture FFN activations\"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # Save activation values\n",
        "        activations[name] = output.detach().cpu()\n",
        "\n",
        "        # Identify activated neurons (neurons exceeding the threshold)\n",
        "        if output.dim() >= 2:\n",
        "            # If it's a 3D tensor [batch_size, seq_len, hidden_dim]\n",
        "            if output.dim() == 3:\n",
        "                # For each sample and each position, find neurons with activation exceeding the threshold\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "            # If it's a 2D tensor [batch_size, hidden_dim]\n",
        "            elif output.dim() == 2:\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "    return hook\n",
        "\n",
        "# 5. Register hooks for the model\n",
        "hooks = []\n",
        "\n",
        "# Analyze the structure of the first layer to understand the location of the activation function\n",
        "sample_layer = model.model.decoder.layers[0]\n",
        "print(\"Layer Structure Analysis:\")\n",
        "for name, module in sample_layer.named_modules():\n",
        "    print(f\"  - {name}: {type(module).__name__}\")\n",
        "\n",
        "# Register hooks for FFN layers - targeting the OPT model architecture\n",
        "for i, layer in enumerate(model.model.decoder.layers):\n",
        "    # Find the activation function in the OPT model\n",
        "    activation_found = False\n",
        "\n",
        "    # Find activation functions in all modules\n",
        "    for name, module in layer.named_modules():\n",
        "        # Look for GELU or ReLU activation functions\n",
        "        if isinstance(module, torch.nn.GELU) or isinstance(module, torch.nn.ReLU):\n",
        "            print(f\"Registering hook for module {name} in layer {i}\")\n",
        "            hook = module.register_forward_hook(get_ffn_activation(f'layer_{i}_{name}'))\n",
        "            hooks.append(hook)\n",
        "            activation_found = True\n",
        "\n",
        "    if not activation_found:\n",
        "        print(f\"Warning: Activation function not found in layer {i}\")\n",
        "\n",
        "# 7. Collect sample activation data\n",
        "def process_sample(sample, sample_id):\n",
        "    \"\"\"Process a single sample and collect activations\"\"\"\n",
        "    # Clear previous activations\n",
        "    activations.clear()\n",
        "    activated_neurons.clear()\n",
        "\n",
        "    # Prepare input for the sample\n",
        "    encodings = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    encodings = {k: v.to(model.device) for k, v in encodings.items()} # Corrected to model.device\n",
        "\n",
        "    # Run the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "\n",
        "    # Collect activated neurons\n",
        "    sample_activated_neurons = {}\n",
        "    for key, value in activated_neurons.items():\n",
        "        try:\n",
        "            if len(value.shape) >= 2:\n",
        "                # Remove batch dimension\n",
        "                mask = value.squeeze(0).numpy()\n",
        "\n",
        "                # For each position, record the indices of activated neurons\n",
        "                if len(mask.shape) == 2:  # [seq_len, hidden_dim]\n",
        "                    position_neurons = {}\n",
        "                    for pos in range(mask.shape[0]):\n",
        "                        active_indices = np.where(mask[pos])[0]\n",
        "                        if len(active_indices) > 0:\n",
        "                            position_neurons[pos] = active_indices.tolist()\n",
        "\n",
        "                    sample_activated_neurons[key] = position_neurons\n",
        "                elif len(mask.shape) == 1:  # [hidden_dim]\n",
        "                    active_indices = np.where(mask)[0]\n",
        "                    if len(active_indices) > 0:\n",
        "                        sample_activated_neurons[key] = {0: active_indices.tolist()}\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    # Return sample information\n",
        "    return {\n",
        "        'sample_id': sample_id,\n",
        "        'text': sample['text'],\n",
        "        'label': sample['label'],\n",
        "        'activated_neurons': sample_activated_neurons,\n",
        "        'input_ids': encodings['input_ids'][0].cpu().numpy(),\n",
        "    }\n",
        "\n",
        "def collect_activations(samples, batch_size=10):\n",
        "    \"\"\"Collect activation data for samples in batches\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Process samples in batches to save memory\n",
        "    for i in range(0, len(samples), batch_size):\n",
        "        batch = samples[i:i+batch_size]\n",
        "        batch_results = []\n",
        "\n",
        "        for j, sample in enumerate(batch):\n",
        "            try:\n",
        "                result = process_sample(sample, i + j)\n",
        "                batch_results.append(result)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "        # Clean up memory\n",
        "        if i + batch_size < len(samples):  # Not the last batch\n",
        "            del batch_results\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# 8. Analyze neuron activation patterns\n",
        "def analyze_neuron_activation_patterns(member_neurons, nonmember_neurons):\n",
        "    \"\"\"Analyze the activation patterns of neurons in member and non-member samples\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Get all layer names\n",
        "    layer_names = set()\n",
        "    for sample in member_neurons + nonmember_neurons:\n",
        "        layer_names.update(sample['activated_neurons'].keys())\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in layer_names:\n",
        "        # Count activated neurons in member and non-member samples\n",
        "        member_neuron_counts = Counter()\n",
        "        nonmember_neuron_counts = Counter()\n",
        "\n",
        "        # Activated neurons in member samples\n",
        "        for sample in member_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    member_neuron_counts.update(pos_neurons)\n",
        "\n",
        "        # Activated neurons in non-member samples\n",
        "        for sample in nonmember_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    nonmember_neuron_counts.update(pos_neurons)\n",
        "\n",
        "        # Calculate the activation frequency for each neuron\n",
        "        member_freq = {n: count / len(member_neurons) for n, count in member_neuron_counts.items()} if member_neurons else {}\n",
        "        nonmember_freq = {n: count / len(nonmember_neurons) for n, count in nonmember_neuron_counts.items()} if nonmember_neurons else {}\n",
        "\n",
        "        # Identify neurons predominantly activated in member samples\n",
        "        member_dominant = {}\n",
        "        for neuron, freq in member_freq.items():\n",
        "            if neuron not in nonmember_freq or freq > nonmember_freq[neuron] * 1.5:\n",
        "                member_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons predominantly activated in non-member samples\n",
        "        nonmember_dominant = {}\n",
        "        for neuron, freq in nonmember_freq.items():\n",
        "            if neuron not in member_freq or freq > member_freq[neuron] * 1.5:\n",
        "                nonmember_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons frequently activated in both types of samples\n",
        "        common_neurons = {}\n",
        "        for neuron in set(member_freq.keys()) & set(nonmember_freq.keys()):\n",
        "            if neuron not in member_dominant and neuron not in nonmember_dominant:\n",
        "                common_neurons[neuron] = (member_freq[neuron], nonmember_freq[neuron])\n",
        "\n",
        "        # Save the results\n",
        "        results[layer_name] = {\n",
        "            'member_dominant': member_dominant,\n",
        "            'nonmember_dominant': nonmember_dominant,\n",
        "            'common_neurons': common_neurons,\n",
        "            'member_counts': member_neuron_counts,\n",
        "            'nonmember_counts': nonmember_neuron_counts,\n",
        "            'member_freq': member_freq,\n",
        "            'nonmember_freq': nonmember_freq\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# 9. Build reference patterns\n",
        "def build_reference_patterns(train_samples):\n",
        "    \"\"\"Build reference activation patterns using training samples\"\"\"\n",
        "    # Separate member and non-member samples\n",
        "    member_samples = [s for s in train_samples if s['label'] == 1]\n",
        "    nonmember_samples = [s for s in train_samples if s['label'] == 0]\n",
        "\n",
        "    # Analyze activation pattern differences\n",
        "    reference_patterns = analyze_neuron_activation_patterns(member_samples, nonmember_samples)\n",
        "\n",
        "    return reference_patterns\n",
        "\n",
        "# 10. Calculate discrimination score for each layer\n",
        "def calculate_layer_discrimination_scores(reference_patterns):\n",
        "    \"\"\"Calculate the discriminative power score for each layer\"\"\"\n",
        "    layer_scores = {}\n",
        "\n",
        "    # Calculate the discriminative power score for each layer\n",
        "    for layer_name, data in reference_patterns.items():\n",
        "        # Simple discrimination score: number of member-dominant neurons minus number of non-member-dominant neurons\n",
        "        # If the score is greater than 0, it indicates the layer is more inclined to identify member samples\n",
        "        member_dominant_count = len(data['member_dominant'])\n",
        "        nonmember_dominant_count = len(data['nonmember_dominant'])\n",
        "\n",
        "        discrimination_score = member_dominant_count - nonmember_dominant_count\n",
        "        layer_scores[layer_name] = discrimination_score\n",
        "\n",
        "    return layer_scores\n",
        "\n",
        "# 11. Select the most discriminative layers\n",
        "def select_discriminative_layers(layer_scores, top_n=10):\n",
        "    \"\"\"Select the most discriminative layers\"\"\"\n",
        "    # Sort by discrimination score\n",
        "    sorted_scores = sorted(layer_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    # Select the top N layers\n",
        "    selected_layers = [layer for layer, score in sorted_scores[:top_n]]\n",
        "\n",
        "    return selected_layers\n",
        "\n",
        "# 12. Membership prediction based on relative ratio\n",
        "def predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers, threshold=1.0):\n",
        "    \"\"\"Predict whether a single sample is a member using the relative ratio method\"\"\"\n",
        "    if not sample['activated_neurons']:\n",
        "        return 0, 0, 0  # If there is no activation data, default to non-member\n",
        "\n",
        "    # Initialize relative ratios\n",
        "    layers_counted = 0\n",
        "    total_member_ratio = 0\n",
        "    total_nonmember_ratio = 0\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in discriminative_layers:\n",
        "        if layer_name not in sample['activated_neurons'] or layer_name not in reference_patterns:\n",
        "            continue\n",
        "\n",
        "        # Get the reference pattern for this layer\n",
        "        layer_data = reference_patterns[layer_name]\n",
        "\n",
        "        # Get all neurons activated by this sample in this layer\n",
        "        sample_neurons = set()\n",
        "        for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "            sample_neurons.update(pos_neurons)\n",
        "\n",
        "        if not sample_neurons:\n",
        "            continue\n",
        "\n",
        "        # Calculate the relative overlap with member-dominant neurons\n",
        "        member_dominant_set = set(layer_data['member_dominant'].keys())\n",
        "        member_overlap = len(sample_neurons.intersection(member_dominant_set))\n",
        "\n",
        "        # Calculate the relative overlap with non-member-dominant neurons\n",
        "        nonmember_dominant_set = set(layer_data['nonmember_dominant'].keys())\n",
        "        nonmember_overlap = len(sample_neurons.intersection(nonmember_dominant_set))\n",
        "\n",
        "        # Calculate relative ratios\n",
        "        member_ratio = member_overlap / len(member_dominant_set) if len(member_dominant_set) > 0 else 0\n",
        "        nonmember_ratio = nonmember_overlap / len(nonmember_dominant_set) if len(nonmember_dominant_set) > 0 else 0\n",
        "\n",
        "        # Accumulate relative ratios\n",
        "        total_member_ratio += member_ratio\n",
        "        total_nonmember_ratio += nonmember_ratio\n",
        "        layers_counted += 1\n",
        "\n",
        "    # If there are no valid layers, default to non-member\n",
        "    if layers_counted == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    # Calculate average relative ratios\n",
        "    avg_member_ratio = total_member_ratio / layers_counted\n",
        "    avg_nonmember_ratio = total_nonmember_ratio / layers_counted\n",
        "\n",
        "    # Calculate the ratio of relative ratios\n",
        "    if avg_nonmember_ratio == 0:\n",
        "        ratio = float('inf')  # Avoid division by zero\n",
        "    else:\n",
        "        ratio = avg_member_ratio / avg_nonmember_ratio\n",
        "\n",
        "    # Return the prediction and related ratios\n",
        "    return 1 if ratio >= threshold else 0, avg_member_ratio, avg_nonmember_ratio\n",
        "\n",
        "# 14. Find the best threshold on the validation set\n",
        "def find_best_relative_ratio_threshold(val_samples, reference_patterns, discriminative_layers):\n",
        "    \"\"\"Find the best threshold on the validation set using the relative ratio method\"\"\"\n",
        "    # Calculate relative ratios for all validation samples\n",
        "    ratios = []\n",
        "    labels = []\n",
        "\n",
        "    for sample in val_samples:\n",
        "        pred, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "            sample, reference_patterns, discriminative_layers, threshold=1.0\n",
        "        )\n",
        "\n",
        "        # Calculate the ratio of relative ratios\n",
        "        ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "        ratios.append(ratio)\n",
        "        labels.append(sample['label'])\n",
        "\n",
        "    # Filter out infinite values\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for r, l in zip(ratios, labels):\n",
        "        if r != float('inf') and not np.isnan(r):\n",
        "            filtered_ratios.append(r)\n",
        "            filtered_labels.append(l)\n",
        "\n",
        "    if not filtered_ratios: # If there are no valid ratios\n",
        "        return 1.0, {'accuracy': 0.5, 'precision': 0, 'recall': 0, 'f1': 0}\n",
        "\n",
        "    # Create candidate thresholds\n",
        "    min_ratio = min(filtered_ratios)\n",
        "    max_ratio = max(filtered_ratios)\n",
        "\n",
        "    # Generate uniformly distributed thresholds\n",
        "    candidate_thresholds = list(np.linspace(min_ratio, max_ratio, 100))\n",
        "    # Add some important threshold points\n",
        "    candidate_thresholds += [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n",
        "    candidate_thresholds = sorted(set(candidate_thresholds))\n",
        "\n",
        "    # Find the best threshold\n",
        "    best_threshold = 1.0\n",
        "    best_accuracy = 0\n",
        "    best_metrics = None\n",
        "\n",
        "    results = []\n",
        "    for threshold in candidate_thresholds:\n",
        "        # Make predictions using the current threshold\n",
        "        predictions = [1 if r >= threshold else 0 for r in ratios]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary', zero_division=0)\n",
        "\n",
        "        # Record the results\n",
        "        results.append({\n",
        "            'threshold': threshold,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        # Update the best threshold\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Calculate AUC and TPR@5%FPR for the test set\n",
        "def calculate_auc_and_tpr(test_samples, reference_patterns, discriminative_layers, batch_size=10):\n",
        "    \"\"\"Calculate the AUC value and TPR@5%FPR for the test set\"\"\"\n",
        "    # Process test samples in batches\n",
        "    all_ratios = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(test_samples), batch_size):\n",
        "        batch = test_samples[i:i+batch_size]\n",
        "\n",
        "        # Collect activation data\n",
        "        batch_acts = collect_activations(batch, batch_size=batch_size)\n",
        "\n",
        "        for sample in batch_acts:\n",
        "            # Calculate relative ratios and predict\n",
        "            _, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "                sample, reference_patterns, discriminative_layers\n",
        "            )\n",
        "\n",
        "            # Calculate the ratio of relative ratios\n",
        "            ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "            # Record the results\n",
        "            all_ratios.append(ratio)\n",
        "            all_labels.append(sample['label'])\n",
        "\n",
        "        # Release memory\n",
        "        del batch_acts\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Filter out infinite values for ROC analysis\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for ratio, label in zip(all_ratios, all_labels):\n",
        "        if ratio != float('inf') and not np.isnan(ratio):\n",
        "            filtered_ratios.append(ratio)\n",
        "            filtered_labels.append(label)\n",
        "\n",
        "    if len(set(filtered_labels)) < 2:\n",
        "        return 0.5, 0.0 # Cannot calculate metrics\n",
        "\n",
        "    # Calculate AUC\n",
        "    fpr, tpr, _ = roc_curve(filtered_labels, filtered_ratios)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Calculate TPR@5%FPR\n",
        "    tpr_at_fpr = 0.0\n",
        "    try:\n",
        "        target_fpr = 0.05\n",
        "        target_index = np.where(fpr >= target_fpr)[0][0]\n",
        "        tpr_at_fpr = tpr[target_index]\n",
        "    except IndexError:\n",
        "        # If FPR never reaches 5%, then TPR@5%FPR is 0 or handled according to the specific case\n",
        "        tpr_at_fpr = 0.0\n",
        "\n",
        "    return roc_auc, tpr_at_fpr\n",
        "\n",
        "# Process all thresholds for a single dataset\n",
        "def process_dataset(data_type, data_dir=\"/content/drive/MyDrive/LLM_MIA/data\", thresholds=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]):\n",
        "    \"\"\"Process all thresholds for a single dataset\"\"\"\n",
        "    global ACTIVATION_THRESHOLD\n",
        "\n",
        "    print(f\"\\n===== Dataset: {data_type} =====\")\n",
        "\n",
        "    # --- Core Change: Remove ds_train and load dev and test based on the data_type condition ---\n",
        "    if data_type == \"prompt\":\n",
        "        dev_path = os.path.join(data_dir, \"pile_cc_mia_dev_prompt_gpt4o_simple_v1\")\n",
        "        test_path = os.path.join(data_dir, \"pile_cc_mia_test_prompt_gpt4o_simple_v1\")\n",
        "        print(f\"Loading specific datasets for 'prompt' type:\")\n",
        "        print(f\"  - DEV: {dev_path}\")\n",
        "        print(f\"  - TEST: {test_path}\")\n",
        "        ds_dev = load_from_disk(dev_path)\n",
        "        ds_test = load_from_disk(test_path)\n",
        "    else:\n",
        "        # Keep the original loading logic for bt and bert\n",
        "        ds_dev = load_from_disk(os.path.join(data_dir, f\"pile_cc_mia_dev_{data_type}\"))\n",
        "        ds_test = load_from_disk(os.path.join(data_dir, f\"pile_cc_mia_test_{data_type}\"))\n",
        "    # --- End of change ---\n",
        "\n",
        "    # Process for each threshold\n",
        "    for threshold in thresholds:\n",
        "        # Set the current threshold\n",
        "        ACTIVATION_THRESHOLD = threshold\n",
        "\n",
        "        # 1. Get member and non-member samples from the development set\n",
        "        dev_member = [item for item in ds_dev if item['label'] == 1]\n",
        "        dev_nonmember = [item for item in ds_dev if item['label'] == 0]\n",
        "\n",
        "        # 2. Split the development set evenly into training and validation sets\n",
        "        train_member = dev_member[:len(dev_member)//2]\n",
        "        train_nonmember = dev_nonmember[:len(dev_nonmember)//2]\n",
        "\n",
        "        val_member = dev_member[len(dev_member)//2:]\n",
        "        val_nonmember = dev_nonmember[len(dev_nonmember)//2:]\n",
        "\n",
        "        # 3. Collect activation data\n",
        "        train_member_acts = collect_activations(train_member)\n",
        "        train_nonmember_acts = collect_activations(train_nonmember)\n",
        "\n",
        "        val_member_acts = collect_activations(val_member)\n",
        "        val_nonmember_acts = collect_activations(val_nonmember)\n",
        "\n",
        "        # 4. Build reference activation patterns\n",
        "        reference_patterns = build_reference_patterns(\n",
        "            train_member_acts + train_nonmember_acts\n",
        "        )\n",
        "\n",
        "        # 5. Calculate layer discrimination scores\n",
        "        layer_scores = calculate_layer_discrimination_scores(reference_patterns)\n",
        "\n",
        "        # 6. Select the most discriminative layers\n",
        "        discriminative_layers = select_discriminative_layers(layer_scores, top_n=10)\n",
        "\n",
        "        # 7. Find the best threshold on the validation set using the relative ratio method\n",
        "        val_samples = val_member_acts + val_nonmember_acts\n",
        "        best_threshold, val_metrics = find_best_relative_ratio_threshold(\n",
        "            val_samples,\n",
        "            reference_patterns,\n",
        "            discriminative_layers\n",
        "        )\n",
        "\n",
        "        # 8. Calculate test set AUC and TPR@5%FPR\n",
        "        test_auc, test_tpr_at_fpr = calculate_auc_and_tpr(list(ds_test), reference_patterns, discriminative_layers)\n",
        "\n",
        "        # 9. Print results\n",
        "        print(f\"Dataset: {data_type}, Threshold: {threshold:.1f}, Test Set AUC = {test_auc:.4f}, TPR@5%FPR (%) = {test_tpr_at_fpr * 100:.2f}%\")\n",
        "\n",
        "        # 10. Clean up memory\n",
        "        del train_member_acts, train_nonmember_acts, val_member_acts, val_nonmember_acts\n",
        "        del reference_patterns, layer_scores, discriminative_layers, val_samples\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Release memory after processing the current dataset\n",
        "    del ds_dev, ds_test\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Set the activation thresholds to be evaluated\n",
        "        activation_thresholds = [1.0]\n",
        "\n",
        "        # Process the three datasets sequentially\n",
        "        datasets = [\"bt\", \"bert\", \"prompt\"]\n",
        "\n",
        "        for dataset in datasets:\n",
        "            process_dataset(dataset, thresholds=activation_thresholds)\n",
        "\n",
        "        # After all datasets are processed, remove the hooks\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {str(e)}\")\n",
        "        # Ensure hooks are removed\n",
        "        for hook in hooks:\n",
        "            try:\n",
        "                hook.remove()\n",
        "            except:\n",
        "                pass\n",
        "        raise e"
      ],
      "metadata": {
        "id": "CwMYxSKBLHBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WikiMia"
      ],
      "metadata": {
        "id": "dRgw6H_2huWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pythia-2.8b"
      ],
      "metadata": {
        "id": "mlHTnHkeLI8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_from_disk, load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_fscore_support\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable tqdm - a safer way\n",
        "import sys\n",
        "import importlib\n",
        "import tqdm as tqdm_module\n",
        "\n",
        "# Save the original module before loading tqdm\n",
        "original_module = sys.modules.get('tqdm', None)\n",
        "\n",
        "# Create a no-op tqdm substitute\n",
        "class DummyTqdmModule:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def update(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def close(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        raise StopIteration\n",
        "\n",
        "# Override all methods of the tqdm class\n",
        "def dummy_tqdm(*args, **kwargs):\n",
        "    if len(args) > 0 and isinstance(args[0], list):\n",
        "        return args[0]\n",
        "    return DummyTqdmModule()\n",
        "\n",
        "# Patch all possible tqdm versions\n",
        "for name in ['tqdm', 'tqdm.std', 'tqdm.auto', 'tqdm.notebook', 'tqdm.rich', 'tqdm.cli', 'tqdm.gui', 'tqdm.keras']:\n",
        "    try:\n",
        "        if name in sys.modules:\n",
        "            module = sys.modules[name]\n",
        "            module.tqdm = dummy_tqdm\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "tqdm_module.tqdm = dummy_tqdm\n",
        "\n",
        "# 1. Set up the environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3. Load the model\n",
        "model_name = \"EleutherAI/pythia-2.8b\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is on the correct device\n",
        "model = model.to(device)\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# If the tokenizer does not have a padding token, set one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 4. Set up hook functions to collect neuron activations\n",
        "activations = {}\n",
        "activated_neurons = {}\n",
        "ACTIVATION_THRESHOLD = 0  # Set activation threshold\n",
        "\n",
        "def get_ffn_activation(name):\n",
        "    \"\"\"Hook function to capture FFN activations\"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # Save activation values\n",
        "        activations[name] = output.detach().cpu()\n",
        "\n",
        "        # Identify activated neurons (neurons exceeding the threshold)\n",
        "        if output.dim() >= 2:\n",
        "            # If it's a 3D tensor [batch_size, seq_len, hidden_dim]\n",
        "            if output.dim() == 3:\n",
        "                # For each sample and each position, find neurons with activation exceeding the threshold\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "            # If it's a 2D tensor [batch_size, hidden_dim]\n",
        "            elif output.dim() == 2:\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "    return hook\n",
        "\n",
        "# 5. Register hooks for the model\n",
        "hooks = []\n",
        "\n",
        "# Register hooks for FFN layers\n",
        "for i, layer in enumerate(model.gpt_neox.layers):\n",
        "    # Register MLP activation function hook\n",
        "    if hasattr(layer.mlp, 'act'):\n",
        "        hook = layer.mlp.act.register_forward_hook(get_ffn_activation(f'layer_{i}_ffn_act'))\n",
        "        hooks.append(hook)\n",
        "    else:\n",
        "        # If the 'act' attribute does not exist, try to find the correct activation layer\n",
        "        for name, module in layer.mlp.named_modules():\n",
        "            if isinstance(module, torch.nn.GELU) or isinstance(module, torch.nn.ReLU):\n",
        "                hook = module.register_forward_hook(get_ffn_activation(f'layer_{i}_ffn_act_{name}'))\n",
        "                hooks.append(hook)\n",
        "                break\n",
        "\n",
        "# 7. Collect sample activation data\n",
        "def process_sample(sample, sample_id):\n",
        "    \"\"\"Process a single sample and collect activations\"\"\"\n",
        "    # Clear previous activations\n",
        "    activations.clear()\n",
        "    activated_neurons.clear()\n",
        "\n",
        "    # Prepare input for the sample\n",
        "    encodings = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "\n",
        "    # Run the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "\n",
        "    # Collect activated neurons\n",
        "    sample_activated_neurons = {}\n",
        "    for key, value in activated_neurons.items():\n",
        "        try:\n",
        "            if len(value.shape) >= 2:\n",
        "                # Remove batch dimension\n",
        "                mask = value.squeeze(0).numpy()\n",
        "\n",
        "                # For each position, record the indices of activated neurons\n",
        "                if len(mask.shape) == 2:  # [seq_len, hidden_dim]\n",
        "                    position_neurons = {}\n",
        "                    for pos in range(mask.shape[0]):\n",
        "                        active_indices = np.where(mask[pos])[0]\n",
        "                        if len(active_indices) > 0:\n",
        "                            position_neurons[pos] = active_indices.tolist()\n",
        "\n",
        "                    sample_activated_neurons[key] = position_neurons\n",
        "                elif len(mask.shape) == 1:  # [hidden_dim]\n",
        "                    active_indices = np.where(mask)[0]\n",
        "                    if len(active_indices) > 0:\n",
        "                        sample_activated_neurons[key] = {0: active_indices.tolist()}\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    # Return sample information\n",
        "    return {\n",
        "        'sample_id': sample_id,\n",
        "        'text': sample['text'],\n",
        "        'label': sample['label'],\n",
        "        'activated_neurons': sample_activated_neurons,\n",
        "        'input_ids': encodings['input_ids'][0].cpu().numpy(),\n",
        "    }\n",
        "\n",
        "def collect_activations(samples, batch_size=10):\n",
        "    \"\"\"Collect activation data for samples in batches\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Process samples in batches to save memory\n",
        "    for i in range(0, len(samples), batch_size):\n",
        "        batch = samples[i:i+batch_size]\n",
        "        batch_results = []\n",
        "\n",
        "        for j, sample in enumerate(batch):\n",
        "            try:\n",
        "                result = process_sample(sample, i + j)\n",
        "                batch_results.append(result)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "        # Clean up memory\n",
        "        if i + batch_size < len(samples):  # Not the last batch\n",
        "            del batch_results\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# 8. Analyze neuron activation patterns\n",
        "def analyze_neuron_activation_patterns(member_neurons, nonmember_neurons):\n",
        "    \"\"\"Analyze the activation patterns of neurons in member and non-member samples\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Get all layer names\n",
        "    layer_names = set()\n",
        "    for sample in member_neurons + nonmember_neurons:\n",
        "        layer_names.update(sample['activated_neurons'].keys())\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in layer_names:\n",
        "        # Count activated neurons in member and non-member samples\n",
        "        member_neuron_counts = Counter()\n",
        "        nonmember_neuron_counts = Counter()\n",
        "\n",
        "        # Activated neurons in member samples\n",
        "        for sample in member_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    member_neuron_counts.update(pos_neurons)\n",
        "\n",
        "        # Activated neurons in non-member samples\n",
        "        for sample in nonmember_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    nonmember_neuron_counts.update(pos_neurons)\n",
        "\n",
        "        # Calculate the activation frequency for each neuron\n",
        "        member_freq = {n: count / len(member_neurons) for n, count in member_neuron_counts.items()}\n",
        "        nonmember_freq = {n: count / len(nonmember_neurons) for n, count in nonmember_neuron_counts.items()}\n",
        "\n",
        "        # Identify neurons predominantly activated in member samples\n",
        "        member_dominant = {}\n",
        "        for neuron, freq in member_freq.items():\n",
        "            if neuron not in nonmember_freq or freq > nonmember_freq[neuron] * 1.5:\n",
        "                member_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons predominantly activated in non-member samples\n",
        "        nonmember_dominant = {}\n",
        "        for neuron, freq in nonmember_freq.items():\n",
        "            if neuron not in member_freq or freq > nonmember_freq[neuron] * 1.5:\n",
        "                nonmember_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons frequently activated in both types of samples\n",
        "        common_neurons = {}\n",
        "        for neuron in set(member_freq.keys()) & set(nonmember_freq.keys()):\n",
        "            if neuron not in member_dominant and neuron not in nonmember_dominant:\n",
        "                common_neurons[neuron] = (member_freq[neuron], nonmember_freq[neuron])\n",
        "\n",
        "        # Save the results\n",
        "        results[layer_name] = {\n",
        "            'member_dominant': member_dominant,\n",
        "            'nonmember_dominant': nonmember_dominant,\n",
        "            'common_neurons': common_neurons,\n",
        "            'member_counts': member_neuron_counts,\n",
        "            'nonmember_counts': nonmember_neuron_counts,\n",
        "            'member_freq': member_freq,\n",
        "            'nonmember_freq': nonmember_freq\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# 9. Build reference patterns\n",
        "def build_reference_patterns(train_samples):\n",
        "    \"\"\"Build reference activation patterns using training samples\"\"\"\n",
        "    # Separate member and non-member samples\n",
        "    member_samples = [s for s in train_samples if s['label'] == 1]\n",
        "    nonmember_samples = [s for s in train_samples if s['label'] == 0]\n",
        "\n",
        "    # Analyze activation pattern differences\n",
        "    reference_patterns = analyze_neuron_activation_patterns(member_samples, nonmember_samples)\n",
        "\n",
        "    return reference_patterns\n",
        "\n",
        "# 10. Calculate discrimination score for each layer\n",
        "def calculate_layer_discrimination_scores(reference_patterns):\n",
        "    \"\"\"Calculate the discriminative power score for each layer\"\"\"\n",
        "    layer_scores = {}\n",
        "\n",
        "    # Calculate the discriminative power score for each layer\n",
        "    for layer_name, data in reference_patterns.items():\n",
        "        # Simple discrimination score: number of member-dominant neurons minus number of non-member-dominant neurons\n",
        "        # If the score is greater than 0, it indicates the layer is more inclined to identify member samples\n",
        "        member_dominant_count = len(data['member_dominant'])\n",
        "        nonmember_dominant_count = len(data['nonmember_dominant'])\n",
        "\n",
        "        discrimination_score = member_dominant_count - nonmember_dominant_count\n",
        "        layer_scores[layer_name] = discrimination_score\n",
        "\n",
        "    return layer_scores\n",
        "\n",
        "# 11. Select the most discriminative layers\n",
        "def select_discriminative_layers(layer_scores, top_n=10):\n",
        "    \"\"\"Select the most discriminative layers\"\"\"\n",
        "    # Sort by discrimination score\n",
        "    sorted_scores = sorted(layer_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    # Select the top N layers\n",
        "    selected_layers = [layer for layer, score in sorted_scores[:top_n]]\n",
        "\n",
        "    return selected_layers\n",
        "\n",
        "# 12. Membership prediction based on relative ratio\n",
        "def predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers, threshold=1.0):\n",
        "    \"\"\"Predict whether a single sample is a member using the relative ratio method\"\"\"\n",
        "    if not sample['activated_neurons']:\n",
        "        return 0, 0, 0  # If there is no activation data, default to non-member\n",
        "\n",
        "    # Initialize relative ratios\n",
        "    layers_counted = 0\n",
        "    total_member_ratio = 0\n",
        "    total_nonmember_ratio = 0\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in discriminative_layers:\n",
        "        if layer_name not in sample['activated_neurons'] or layer_name not in reference_patterns:\n",
        "            continue\n",
        "\n",
        "        # Get the reference pattern for this layer\n",
        "        layer_data = reference_patterns[layer_name]\n",
        "\n",
        "        # Get all neurons activated by this sample in this layer\n",
        "        sample_neurons = set()\n",
        "        for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "            sample_neurons.update(pos_neurons)\n",
        "\n",
        "        if not sample_neurons:\n",
        "            continue\n",
        "\n",
        "        # Calculate the relative overlap with member-dominant neurons\n",
        "        member_dominant_set = set(layer_data['member_dominant'].keys())\n",
        "        member_overlap = len(sample_neurons.intersection(member_dominant_set))\n",
        "\n",
        "        # Calculate the relative overlap with non-member-dominant neurons\n",
        "        nonmember_dominant_set = set(layer_data['nonmember_dominant'].keys())\n",
        "        nonmember_overlap = len(sample_neurons.intersection(nonmember_dominant_set))\n",
        "\n",
        "        # Calculate relative ratios\n",
        "        member_ratio = member_overlap / len(member_dominant_set) if len(member_dominant_set) > 0 else 0\n",
        "        nonmember_ratio = nonmember_overlap / len(nonmember_dominant_set) if len(nonmember_dominant_set) > 0 else 0\n",
        "\n",
        "        # Accumulate relative ratios\n",
        "        total_member_ratio += member_ratio\n",
        "        total_nonmember_ratio += nonmember_ratio\n",
        "        layers_counted += 1\n",
        "\n",
        "    # If there are no valid layers, default to non-member\n",
        "    if layers_counted == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    # Calculate average relative ratios\n",
        "    avg_member_ratio = total_member_ratio / layers_counted\n",
        "    avg_nonmember_ratio = total_nonmember_ratio / layers_counted\n",
        "\n",
        "    # Calculate the ratio of relative ratios\n",
        "    if avg_nonmember_ratio == 0:\n",
        "        ratio = float('inf')  # Avoid division by zero\n",
        "    else:\n",
        "        ratio = avg_member_ratio / avg_nonmember_ratio\n",
        "\n",
        "    # Return the prediction and related ratios\n",
        "    return 1 if ratio >= threshold else 0, avg_member_ratio, avg_nonmember_ratio\n",
        "\n",
        "# 14. Find the best threshold on the validation set\n",
        "def find_best_relative_ratio_threshold(val_samples, reference_patterns, discriminative_layers):\n",
        "    \"\"\"Find the best threshold on the validation set using the relative ratio method\"\"\"\n",
        "    # Calculate relative ratios for all validation samples\n",
        "    ratios = []\n",
        "    labels = []\n",
        "\n",
        "    for sample in val_samples:\n",
        "        pred, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "            sample, reference_patterns, discriminative_layers, threshold=1.0\n",
        "        )\n",
        "\n",
        "        # Calculate the ratio of relative ratios\n",
        "        ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "        ratios.append(ratio)\n",
        "        labels.append(sample['label'])\n",
        "\n",
        "    # Filter out infinite values\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for r, l in zip(ratios, labels):\n",
        "        if r != float('inf') and not np.isnan(r):\n",
        "            filtered_ratios.append(r)\n",
        "            filtered_labels.append(l)\n",
        "\n",
        "    # Create candidate thresholds\n",
        "    min_ratio = min(filtered_ratios)\n",
        "    max_ratio = max(filtered_ratios)\n",
        "\n",
        "    # Generate uniformly distributed thresholds\n",
        "    candidate_thresholds = list(np.linspace(min_ratio, max_ratio, 100))\n",
        "    # Add some important threshold points\n",
        "    candidate_thresholds += [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n",
        "    candidate_thresholds = sorted(set(candidate_thresholds))\n",
        "\n",
        "    # Find the best threshold\n",
        "    best_threshold = 1.0\n",
        "    best_accuracy = 0\n",
        "    best_metrics = None\n",
        "\n",
        "    results = []\n",
        "    for threshold in candidate_thresholds:\n",
        "        # Make predictions using the current threshold\n",
        "        predictions = [1 if r >= threshold else 0 for r in ratios]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "\n",
        "        # Record the results\n",
        "        results.append({\n",
        "            'threshold': threshold,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        # Update the best threshold\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Calculate AUC for the test set\n",
        "def calculate_auc(test_samples, reference_patterns, discriminative_layers, batch_size=10):\n",
        "    \"\"\"Calculate the AUC value for the test set\"\"\"\n",
        "    # Process test samples in batches\n",
        "    all_ratios = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(test_samples), batch_size):\n",
        "        batch = test_samples[i:i+batch_size]\n",
        "\n",
        "        # Collect activation data\n",
        "        batch_acts = collect_activations(batch, batch_size=batch_size)\n",
        "\n",
        "        for sample in batch_acts:\n",
        "            # Calculate relative ratios and predict\n",
        "            _, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "                sample, reference_patterns, discriminative_layers\n",
        "            )\n",
        "\n",
        "            # Calculate the ratio of relative ratios\n",
        "            ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "            # Record the results\n",
        "            all_ratios.append(ratio)\n",
        "            all_labels.append(sample['label'])\n",
        "\n",
        "        # Release memory\n",
        "        del batch_acts\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Filter out infinite values for ROC analysis\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for ratio, label in zip(all_ratios, all_labels):\n",
        "        if ratio != float('inf') and not np.isnan(ratio):\n",
        "            filtered_ratios.append(ratio)\n",
        "            filtered_labels.append(label)\n",
        "\n",
        "    # Calculate AUC\n",
        "    fpr, tpr, _ = roc_curve(filtered_labels, filtered_ratios)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "# Simply iterate over samples without using tqdm\n",
        "def iterate_samples(samples):\n",
        "    \"\"\"Simply iterate over samples, avoiding tqdm\"\"\"\n",
        "    return samples\n",
        "\n",
        "# Process WikiMIA dataset\n",
        "def process_wikimia(thresholds=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]):\n",
        "    \"\"\"Process all thresholds for the WikiMIA dataset\"\"\"\n",
        "    global ACTIVATION_THRESHOLD\n",
        "\n",
        "    print(\"\\n===== Dataset: WikiMIA =====\")\n",
        "\n",
        "    # Load the WikiMIA dataset\n",
        "    try:\n",
        "        # First, try to load the prepared dataset directly\n",
        "        ds_dev = load_from_disk(\"data/ds_dev\")\n",
        "        ds_test = load_from_disk(\"data/ds_test\")\n",
        "        print(\"Loaded prepared dataset from disk\")\n",
        "    except:\n",
        "        # If the prepared dataset is not found, process from scratch\n",
        "        print(\"Prepared dataset not found, processing WikiMIA data from scratch...\")\n",
        "\n",
        "        # Load the WikiMIA dataset\n",
        "        ds = load_dataset(\"swj0419/WikiMIA\")\n",
        "\n",
        "        # Select only the length32 data\n",
        "        length32_data = ds[\"WikiMIA_length32\"]\n",
        "\n",
        "        # Separate member (label=1) and non-member (label=0) samples, and rename the 'input' field to 'text'\n",
        "        member_samples = []\n",
        "        nonmember_samples = []\n",
        "\n",
        "        for item in length32_data:\n",
        "            # Create a new sample dictionary, changing the 'input' field to 'text'\n",
        "            new_item = {\n",
        "                'text': item['input'],\n",
        "                'label': item['label']\n",
        "            }\n",
        "            # Keep other existing fields\n",
        "            for key, value in item.items():\n",
        "                if key not in ['input', 'text', 'label']:\n",
        "                    new_item[key] = value\n",
        "\n",
        "            # Classify based on the label\n",
        "            if item['label'] == 1:\n",
        "                member_samples.append(new_item)\n",
        "            else:\n",
        "                nonmember_samples.append(new_item)\n",
        "\n",
        "        print(f\"Number of member samples: {len(member_samples)}\")\n",
        "        print(f\"Number of non-member samples: {len(nonmember_samples)}\")\n",
        "\n",
        "        # Adjust dataset sizes\n",
        "        # Use the number of available samples to determine the split\n",
        "        available_member_count = len(member_samples)\n",
        "        available_nonmember_count = len(nonmember_samples)\n",
        "\n",
        "        # Set the number of member samples in the dev and test sets\n",
        "        # Considering there are only 387 member samples, we can take 140 for dev and the remaining 247 for test\n",
        "        dev_member_count = 140\n",
        "        test_member_count = available_member_count - dev_member_count  # should be 247\n",
        "\n",
        "        # Set the same number of non-member samples to maintain balance\n",
        "        dev_nonmember_count = dev_member_count\n",
        "        test_nonmember_count = test_member_count\n",
        "\n",
        "        # Set a random seed for reproducibility\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Split the member samples\n",
        "        dev_members = member_samples[:dev_member_count]\n",
        "        test_members = member_samples[dev_member_count:]\n",
        "\n",
        "        # Split the non-member samples\n",
        "        dev_nonmembers = nonmember_samples[:dev_nonmember_count]\n",
        "        test_nonmembers = nonmember_samples[dev_nonmember_count:dev_nonmember_count+test_nonmember_count]\n",
        "\n",
        "        # Combine the dev set\n",
        "        ds_dev_list = dev_members + dev_nonmembers\n",
        "        np.random.shuffle(ds_dev_list)  # Shuffle the order\n",
        "\n",
        "        # Combine the test set\n",
        "        ds_test_list = test_members + test_nonmembers\n",
        "        np.random.shuffle(ds_test_list)  # Shuffle the order\n",
        "\n",
        "        # Verify dataset sizes and label balance\n",
        "        print(f\"Dev set size: {len(ds_dev_list)}\")\n",
        "        print(f\"Number of member samples in Dev set: {sum(1 for item in ds_dev_list if item['label'] == 1)}\")\n",
        "        print(f\"Number of non-member samples in Dev set: {sum(1 for item in ds_dev_list if item['label'] == 0)}\")\n",
        "\n",
        "        print(f\"Test set size: {len(ds_test_list)}\")\n",
        "        print(f\"Number of member samples in Test set: {sum(1 for item in ds_test_list if item['label'] == 1)}\")\n",
        "        print(f\"Number of non-member samples in Test set: {sum(1 for item in ds_test_list if item['label'] == 0)}\")\n",
        "\n",
        "        # Convert to Dataset format\n",
        "        ds_dev = Dataset.from_list(ds_dev_list)\n",
        "        ds_test = Dataset.from_list(ds_test_list)\n",
        "\n",
        "        # Create the data directory if it doesn't exist\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "        # Save to disk\n",
        "        ds_dev.save_to_disk(\"data/ds_dev\")\n",
        "        ds_test.save_to_disk(\"data/ds_test\")\n",
        "\n",
        "        print(\"Datasets have been saved to the data/ds_dev and data/ds_test directories\")\n",
        "\n",
        "    # Process for each threshold\n",
        "    for threshold in thresholds:\n",
        "        # Set the current threshold\n",
        "        ACTIVATION_THRESHOLD = threshold\n",
        "\n",
        "        # 1. Get member and non-member samples from the development set\n",
        "        dev_member = [item for item in ds_dev if item['label'] == 1]\n",
        "        dev_nonmember = [item for item in ds_dev if item['label'] == 0]\n",
        "\n",
        "        # 2. Split the development set evenly into training and validation sets\n",
        "        train_member = dev_member[:len(dev_member)//2]\n",
        "        train_nonmember = dev_nonmember[:len(dev_nonmember)//2]\n",
        "\n",
        "        val_member = dev_member[len(dev_member)//2:]\n",
        "        val_nonmember = dev_nonmember[len(dev_nonmember)//2:]\n",
        "\n",
        "        # 3. Collect activation data\n",
        "        train_member_acts = collect_activations(train_member)\n",
        "        train_nonmember_acts = collect_activations(train_nonmember)\n",
        "\n",
        "        val_member_acts = collect_activations(val_member)\n",
        "        val_nonmember_acts = collect_activations(val_nonmember)\n",
        "\n",
        "        # 4. Build reference activation patterns\n",
        "        reference_patterns = build_reference_patterns(\n",
        "            train_member_acts + train_nonmember_acts\n",
        "        )\n",
        "\n",
        "        # 5. Calculate layer discrimination scores\n",
        "        layer_scores = calculate_layer_discrimination_scores(reference_patterns)\n",
        "\n",
        "        # 6. Select the most discriminative layers\n",
        "        discriminative_layers = select_discriminative_layers(layer_scores, top_n=10)\n",
        "\n",
        "        # 7. Find the best threshold on the validation set using the relative ratio method\n",
        "        val_samples = val_member_acts + val_nonmember_acts\n",
        "        best_threshold, val_metrics = find_best_relative_ratio_threshold(\n",
        "            val_samples,\n",
        "            reference_patterns,\n",
        "            discriminative_layers\n",
        "        )\n",
        "\n",
        "        # 8. Calculate test set AUC\n",
        "        test_auc = calculate_auc(list(ds_test), reference_patterns, discriminative_layers)\n",
        "\n",
        "        # 9. Print only the dataset, threshold, and test set AUC value\n",
        "        print(f\"Dataset: WikiMIA, Threshold: {threshold:.1f}, Test Set AUC = {test_auc:.4f}\")\n",
        "\n",
        "        # 10. Clean up memory\n",
        "        del train_member_acts, train_nonmember_acts, val_member_acts, val_nonmember_acts\n",
        "        del reference_patterns, layer_scores, discriminative_layers, val_samples\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Release memory after processing the current dataset\n",
        "    del ds_dev, ds_test\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Set the activation thresholds to be evaluated\n",
        "        activation_thresholds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
        "\n",
        "        # Only process the WikiMIA dataset\n",
        "        process_wikimia(thresholds=activation_thresholds)\n",
        "\n",
        "        # After processing, remove the hooks\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {str(e)}\")\n",
        "        # Ensure hooks are removed\n",
        "        for hook in hooks:\n",
        "            try:\n",
        "                hook.remove()\n",
        "            except:\n",
        "                pass\n",
        "        raise e"
      ],
      "metadata": {
        "id": "GK6NDks4hxO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Opt-6.7b"
      ],
      "metadata": {
        "id": "-LPiL72rLKmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datasets import load_from_disk, load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_recall_fscore_support\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Disable tqdm - a safer way\n",
        "import sys\n",
        "import importlib\n",
        "# Completely disable tqdm\n",
        "import tqdm\n",
        "\n",
        "# Define a function that does nothing and returns its input\n",
        "def dummy_tqdm(iterable=None, *args, **kwargs):\n",
        "    return iterable if iterable is not None else dummy_tqdm\n",
        "\n",
        "# Add all necessary attributes and methods\n",
        "dummy_tqdm.format_interval = lambda x: f\"{x:.1f}s\"\n",
        "dummy_tqdm.format_meter = lambda *args, **kwargs: \"\"\n",
        "dummy_tqdm.format_num = lambda x: str(x)\n",
        "dummy_tqdm.status_printer = lambda *args, **kwargs: lambda x: None\n",
        "dummy_tqdm.get_lock = lambda: None\n",
        "dummy_tqdm.set_lock = lambda x: None\n",
        "dummy_tqdm.display = lambda *args, **kwargs: None\n",
        "dummy_tqdm.clear = lambda *args, **kwargs: None\n",
        "dummy_tqdm.close = lambda *args, **kwargs: None\n",
        "dummy_tqdm.update = lambda *args, **kwargs: None\n",
        "dummy_tqdm.refresh = lambda *args, **kwargs: None\n",
        "dummy_tqdm.disable = True\n",
        "dummy_tqdm.monitor_interval = 0\n",
        "dummy_tqdm.monitor = None\n",
        "dummy_tqdm.pos = 0\n",
        "dummy_tqdm.__iter__ = lambda self: iter([])\n",
        "dummy_tqdm.__next__ = lambda self: next(iter([]))\n",
        "\n",
        "# Replace all tqdm variants\n",
        "tqdm.tqdm = dummy_tqdm\n",
        "tqdm.std.tqdm = dummy_tqdm\n",
        "tqdm.notebook.tqdm = dummy_tqdm\n",
        "tqdm.auto.tqdm = dummy_tqdm\n",
        "tqdm.gui.tqdm = dummy_tqdm\n",
        "tqdm.cli.tqdm = dummy_tqdm\n",
        "tqdm.__call__ = dummy_tqdm\n",
        "\n",
        "# --- Patch missing tqdm.format_sizeof --- #\n",
        "def _dummy_format_sizeof(num_bytes, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    A fake tqdm.format_sizeof.\n",
        "    Just returns a simple string of the byte count, enough to trick transformers,\n",
        "    without affecting your complete disabling of tqdm.\n",
        "    \"\"\"\n",
        "    return f\"{num_bytes}\"\n",
        "\n",
        "# If you already have a dummy_tqdm object:\n",
        "try:\n",
        "    dummy_tqdm.format_sizeof = _dummy_format_sizeof\n",
        "except NameError:\n",
        "    pass  # Skip if dummy_tqdm does not exist\n",
        "\n",
        "# Also patch the real tqdm module (or the one you replaced)\n",
        "import sys\n",
        "if 'tqdm' in sys.modules:\n",
        "    setattr(sys.modules['tqdm'], 'format_sizeof', _dummy_format_sizeof)\n",
        "\n",
        "# 1. Set up the environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the OPT-6.7B model with 16-bit precision (half-precision, FP16)\n",
        "model_name = \"facebook/opt-6.7b\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Specify 16-bit precision\n",
        "    device_map=\"auto\"           # Automatically manage model distribution on GPU/CPU\n",
        ")\n",
        "\n",
        "# Load the corresponding tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# If the tokenizer does not have a padding token, set one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded, using {model.dtype} precision\")\n",
        "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Display model memory usage\n",
        "def get_model_size(model):\n",
        "    \"\"\"Calculate model size (GB)\"\"\"\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**3\n",
        "    return model_size\n",
        "\n",
        "model_size_gb = get_model_size(model)\n",
        "print(f\"Model size: {model_size_gb:.2f} GB\")\n",
        "\n",
        "# If the tokenizer does not have a padding token, set one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 4. Set up hook functions to collect neuron activations\n",
        "activations = {}\n",
        "activated_neurons = {}\n",
        "ACTIVATION_THRESHOLD = 0  # Set activation threshold\n",
        "\n",
        "def get_ffn_activation(name):\n",
        "    \"\"\"Hook function to capture FFN activations\"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # Save activation values\n",
        "        activations[name] = output.detach().cpu()\n",
        "\n",
        "        # Identify activated neurons (neurons exceeding the threshold)\n",
        "        if output.dim() >= 2:\n",
        "            # If it's a 3D tensor [batch_size, seq_len, hidden_dim]\n",
        "            if output.dim() == 3:\n",
        "                # For each sample and each position, find neurons with activation exceeding the threshold\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "            # If it's a 2D tensor [batch_size, hidden_dim]\n",
        "            elif output.dim() == 2:\n",
        "                activation_mask = (output > ACTIVATION_THRESHOLD).detach().cpu()\n",
        "                activated_neurons[name] = activation_mask\n",
        "    return hook\n",
        "\n",
        "# 5. Register hooks for the model\n",
        "hooks = []\n",
        "\n",
        "# Analyze the structure of the first layer to understand the location of the activation function\n",
        "sample_layer = model.model.decoder.layers[0]\n",
        "print(\"Layer Structure Analysis:\")\n",
        "for name, module in sample_layer.named_modules():\n",
        "    print(f\"  - {name}: {type(module).__name__}\")\n",
        "\n",
        "# Register hooks for FFN layers - targeting the OPT model architecture\n",
        "for i, layer in enumerate(model.model.decoder.layers):\n",
        "    # Find the activation function in the OPT model\n",
        "    activation_found = False\n",
        "\n",
        "    # Find activation functions in all modules\n",
        "    for name, module in layer.named_modules():\n",
        "        # Look for GELU or ReLU activation functions\n",
        "        if isinstance(module, torch.nn.GELU) or isinstance(module, torch.nn.ReLU):\n",
        "            print(f\"Registering hook for module {name} in layer {i}\")\n",
        "            hook = module.register_forward_hook(get_ffn_activation(f'layer_{i}_{name}'))\n",
        "            hooks.append(hook)\n",
        "            activation_found = True\n",
        "\n",
        "    if not activation_found:\n",
        "        print(f\"Warning: Activation function not found in layer {i}\")\n",
        "\n",
        "# 7. Collect sample activation data\n",
        "def process_sample(sample, sample_id):\n",
        "    \"\"\"Process a single sample and collect activations\"\"\"\n",
        "    # Clear previous activations\n",
        "    activations.clear()\n",
        "    activated_neurons.clear()\n",
        "\n",
        "    # Prepare input for the sample\n",
        "    encodings = tokenizer(sample['text'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "\n",
        "    # Run the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "\n",
        "    # Collect activated neurons\n",
        "    sample_activated_neurons = {}\n",
        "    for key, value in activated_neurons.items():\n",
        "        try:\n",
        "            if len(value.shape) >= 2:\n",
        "                # Remove batch dimension\n",
        "                mask = value.squeeze(0).numpy()\n",
        "\n",
        "                # For each position, record the indices of activated neurons\n",
        "                if len(mask.shape) == 2:  # [seq_len, hidden_dim]\n",
        "                    position_neurons = {}\n",
        "                    for pos in range(mask.shape[0]):\n",
        "                        active_indices = np.where(mask[pos])[0]\n",
        "                        if len(active_indices) > 0:\n",
        "                            position_neurons[pos] = active_indices.tolist()\n",
        "\n",
        "                    sample_activated_neurons[key] = position_neurons\n",
        "                elif len(mask.shape) == 1:  # [hidden_dim]\n",
        "                    active_indices = np.where(mask)[0]\n",
        "                    if len(active_indices) > 0:\n",
        "                        sample_activated_neurons[key] = {0: active_indices.tolist()}\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    # Return sample information\n",
        "    return {\n",
        "        'sample_id': sample_id,\n",
        "        'text': sample['text'],\n",
        "        'label': sample['label'],\n",
        "        'activated_neurons': sample_activated_neurons,\n",
        "        'input_ids': encodings['input_ids'][0].cpu().numpy(),\n",
        "    }\n",
        "\n",
        "def collect_activations(samples, batch_size=10):\n",
        "    \"\"\"Collect activation data for samples in batches\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Process samples in batches to save memory\n",
        "    for i in range(0, len(samples), batch_size):\n",
        "        batch = samples[i:i+batch_size]\n",
        "        batch_results = []\n",
        "\n",
        "        for j, sample in enumerate(batch):\n",
        "            try:\n",
        "                result = process_sample(sample, i + j)\n",
        "                batch_results.append(result)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "        # Clean up memory\n",
        "        if i + batch_size < len(samples):  # Not the last batch\n",
        "            del batch_results\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# 8. Analyze neuron activation patterns\n",
        "def analyze_neuron_activation_patterns(member_neurons, nonmember_neurons):\n",
        "    \"\"\"Analyze the activation patterns of neurons in member and non-member samples\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Get all layer names\n",
        "    layer_names = set()\n",
        "    for sample in member_neurons + nonmember_neurons:\n",
        "        layer_names.update(sample['activated_neurons'].keys())\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in layer_names:\n",
        "        # Count activated neurons in member and non-member samples\n",
        "        member_neuron_counts = Counter()\n",
        "        nonmember_neuron_counts = Counter()\n",
        "\n",
        "        # Activated neurons in member samples\n",
        "        for sample in member_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    member_neuron_counts.update(pos_neurons)\n",
        "\n",
        "        # Activated neurons in non-member samples\n",
        "        for sample in nonmember_neurons:\n",
        "            if layer_name in sample['activated_neurons']:\n",
        "                for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "                    nonmember_neuron_counts.update(pos_neurons)\n",
        "\n",
        "        # Calculate the activation frequency for each neuron\n",
        "        member_freq = {n: count / len(member_neurons) for n, count in member_neuron_counts.items()}\n",
        "        nonmember_freq = {n: count / len(nonmember_neurons) for n, count in nonmember_neuron_counts.items()}\n",
        "\n",
        "        # Identify neurons predominantly activated in member samples\n",
        "        member_dominant = {}\n",
        "        for neuron, freq in member_freq.items():\n",
        "            if neuron not in nonmember_freq or freq > nonmember_freq[neuron] * 1.5:\n",
        "                member_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons predominantly activated in non-member samples\n",
        "        nonmember_dominant = {}\n",
        "        for neuron, freq in nonmember_freq.items():\n",
        "            if neuron not in member_freq or freq > member_freq[neuron] * 1.5:\n",
        "                nonmember_dominant[neuron] = freq\n",
        "\n",
        "        # Identify neurons frequently activated in both types of samples\n",
        "        common_neurons = {}\n",
        "        for neuron in set(member_freq.keys()) & set(nonmember_freq.keys()):\n",
        "            if neuron not in member_dominant and neuron not in nonmember_dominant:\n",
        "                common_neurons[neuron] = (member_freq[neuron], nonmember_freq[neuron])\n",
        "\n",
        "        # Save the results\n",
        "        results[layer_name] = {\n",
        "            'member_dominant': member_dominant,\n",
        "            'nonmember_dominant': nonmember_dominant,\n",
        "            'common_neurons': common_neurons,\n",
        "            'member_counts': member_neuron_counts,\n",
        "            'nonmember_counts': nonmember_neuron_counts,\n",
        "            'member_freq': member_freq,\n",
        "            'nonmember_freq': nonmember_freq\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# 9. Build reference patterns\n",
        "def build_reference_patterns(train_samples):\n",
        "    \"\"\"Build reference activation patterns using training samples\"\"\"\n",
        "    # Separate member and non-member samples\n",
        "    member_samples = [s for s in train_samples if s['label'] == 1]\n",
        "    nonmember_samples = [s for s in train_samples if s['label'] == 0]\n",
        "\n",
        "    # Analyze activation pattern differences\n",
        "    reference_patterns = analyze_neuron_activation_patterns(member_samples, nonmember_samples)\n",
        "\n",
        "    return reference_patterns\n",
        "\n",
        "# 10. Calculate discrimination score for each layer\n",
        "def calculate_layer_discrimination_scores(reference_patterns):\n",
        "    \"\"\"Calculate the discriminative power score for each layer\"\"\"\n",
        "    layer_scores = {}\n",
        "\n",
        "    # Calculate the discriminative power score for each layer\n",
        "    for layer_name, data in reference_patterns.items():\n",
        "        # Simple discrimination score: number of member-dominant neurons minus number of non-member-dominant neurons\n",
        "        # If the score is greater than 0, it indicates the layer is more inclined to identify member samples\n",
        "        member_dominant_count = len(data['member_dominant'])\n",
        "        nonmember_dominant_count = len(data['nonmember_dominant'])\n",
        "\n",
        "        discrimination_score = member_dominant_count - nonmember_dominant_count\n",
        "        layer_scores[layer_name] = discrimination_score\n",
        "\n",
        "    return layer_scores\n",
        "\n",
        "# 11. Select the most discriminative layers\n",
        "def select_discriminative_layers(layer_scores, top_n=10):\n",
        "    \"\"\"Select the most discriminative layers\"\"\"\n",
        "    # Sort by discrimination score\n",
        "    sorted_scores = sorted(layer_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    # Select the top N layers\n",
        "    selected_layers = [layer for layer, score in sorted_scores[:top_n]]\n",
        "\n",
        "    return selected_layers\n",
        "\n",
        "# 12. Membership prediction based on relative ratio\n",
        "def predict_membership_by_relative_ratio(sample, reference_patterns, discriminative_layers, threshold=1.0):\n",
        "    \"\"\"Predict whether a single sample is a member using the relative ratio method\"\"\"\n",
        "    if not sample['activated_neurons']:\n",
        "        return 0, 0, 0  # If there is no activation data, default to non-member\n",
        "\n",
        "    # Initialize relative ratios\n",
        "    layers_counted = 0\n",
        "    total_member_ratio = 0\n",
        "    total_nonmember_ratio = 0\n",
        "\n",
        "    # Analyze layer by layer\n",
        "    for layer_name in discriminative_layers:\n",
        "        if layer_name not in sample['activated_neurons'] or layer_name not in reference_patterns:\n",
        "            continue\n",
        "\n",
        "        # Get the reference pattern for this layer\n",
        "        layer_data = reference_patterns[layer_name]\n",
        "\n",
        "        # Get all neurons activated by this sample in this layer\n",
        "        sample_neurons = set()\n",
        "        for pos_neurons in sample['activated_neurons'][layer_name].values():\n",
        "            sample_neurons.update(pos_neurons)\n",
        "\n",
        "        if not sample_neurons:\n",
        "            continue\n",
        "\n",
        "        # Calculate the relative overlap with member-dominant neurons\n",
        "        member_dominant_set = set(layer_data['member_dominant'].keys())\n",
        "        member_overlap = len(sample_neurons.intersection(member_dominant_set))\n",
        "\n",
        "        # Calculate the relative overlap with non-member-dominant neurons\n",
        "        nonmember_dominant_set = set(layer_data['nonmember_dominant'].keys())\n",
        "        nonmember_overlap = len(sample_neurons.intersection(nonmember_dominant_set))\n",
        "\n",
        "        # Calculate relative ratios\n",
        "        member_ratio = member_overlap / len(member_dominant_set) if len(member_dominant_set) > 0 else 0\n",
        "        nonmember_ratio = nonmember_overlap / len(nonmember_dominant_set) if len(nonmember_dominant_set) > 0 else 0\n",
        "\n",
        "        # Accumulate relative ratios\n",
        "        total_member_ratio += member_ratio\n",
        "        total_nonmember_ratio += nonmember_ratio\n",
        "        layers_counted += 1\n",
        "\n",
        "    # If there are no valid layers, default to non-member\n",
        "    if layers_counted == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    # Calculate average relative ratios\n",
        "    avg_member_ratio = total_member_ratio / layers_counted\n",
        "    avg_nonmember_ratio = total_nonmember_ratio / layers_counted\n",
        "\n",
        "    # Calculate the ratio of relative ratios\n",
        "    if avg_nonmember_ratio == 0:\n",
        "        ratio = float('inf')  # Avoid division by zero\n",
        "    else:\n",
        "        ratio = avg_member_ratio / avg_nonmember_ratio\n",
        "\n",
        "    # Return the prediction and related ratios\n",
        "    return 1 if ratio >= threshold else 0, avg_member_ratio, avg_nonmember_ratio\n",
        "\n",
        "# 14. Find the best threshold on the validation set\n",
        "def find_best_relative_ratio_threshold(val_samples, reference_patterns, discriminative_layers):\n",
        "    \"\"\"Find the best threshold on the validation set using the relative ratio method\"\"\"\n",
        "    # Calculate relative ratios for all validation samples\n",
        "    ratios = []\n",
        "    labels = []\n",
        "\n",
        "    for sample in val_samples:\n",
        "        pred, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "            sample, reference_patterns, discriminative_layers, threshold=1.0\n",
        "        )\n",
        "\n",
        "        # Calculate the ratio of relative ratios\n",
        "        ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "        ratios.append(ratio)\n",
        "        labels.append(sample['label'])\n",
        "\n",
        "    # Filter out infinite values\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for r, l in zip(ratios, labels):\n",
        "        if r != float('inf') and not np.isnan(r):\n",
        "            filtered_ratios.append(r)\n",
        "            filtered_labels.append(l)\n",
        "\n",
        "    # Create candidate thresholds\n",
        "    min_ratio = min(filtered_ratios)\n",
        "    max_ratio = max(filtered_ratios)\n",
        "\n",
        "    # Generate uniformly distributed thresholds\n",
        "    candidate_thresholds = list(np.linspace(min_ratio, max_ratio, 100))\n",
        "    # Add some important threshold points\n",
        "    candidate_thresholds += [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n",
        "    candidate_thresholds = sorted(set(candidate_thresholds))\n",
        "\n",
        "    # Find the best threshold\n",
        "    best_threshold = 1.0\n",
        "    best_accuracy = 0\n",
        "    best_metrics = None\n",
        "\n",
        "    results = []\n",
        "    for threshold in candidate_thresholds:\n",
        "        # Make predictions using the current threshold\n",
        "        predictions = [1 if r >= threshold else 0 for r in ratios]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "\n",
        "        # Record the results\n",
        "        results.append({\n",
        "            'threshold': threshold,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        # Update the best threshold\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Calculate AUC for the test set\n",
        "def calculate_auc(test_samples, reference_patterns, discriminative_layers, batch_size=10):\n",
        "    \"\"\"Calculate the AUC value for the test set\"\"\"\n",
        "    # Process test samples in batches\n",
        "    all_ratios = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(test_samples), batch_size):\n",
        "        batch = test_samples[i:i+batch_size]\n",
        "\n",
        "        # Collect activation data\n",
        "        batch_acts = collect_activations(batch, batch_size=batch_size)\n",
        "\n",
        "        for sample in batch_acts:\n",
        "            # Calculate relative ratios and predict\n",
        "            _, member_ratio, nonmember_ratio = predict_membership_by_relative_ratio(\n",
        "                sample, reference_patterns, discriminative_layers\n",
        "            )\n",
        "\n",
        "            # Calculate the ratio of relative ratios\n",
        "            ratio = float('inf') if nonmember_ratio == 0 else member_ratio / nonmember_ratio\n",
        "\n",
        "            # Record the results\n",
        "            all_ratios.append(ratio)\n",
        "            all_labels.append(sample['label'])\n",
        "\n",
        "        # Release memory\n",
        "        del batch_acts\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Filter out infinite values for ROC analysis\n",
        "    filtered_ratios = []\n",
        "    filtered_labels = []\n",
        "    for ratio, label in zip(all_ratios, all_labels):\n",
        "        if ratio != float('inf') and not np.isnan(ratio):\n",
        "            filtered_ratios.append(ratio)\n",
        "            filtered_labels.append(label)\n",
        "\n",
        "    # Calculate AUC\n",
        "    fpr, tpr, _ = roc_curve(filtered_labels, filtered_ratios)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "# Simply iterate over samples without using tqdm\n",
        "def iterate_samples(samples):\n",
        "    \"\"\"Simply iterate over samples, avoiding tqdm\"\"\"\n",
        "    return samples\n",
        "\n",
        "# Process WikiMIA dataset\n",
        "def process_wikimia(thresholds=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]):\n",
        "    \"\"\"Process all thresholds for the WikiMIA dataset\"\"\"\n",
        "    global ACTIVATION_THRESHOLD\n",
        "\n",
        "    print(\"\\n===== Dataset: WikiMIA =====\")\n",
        "\n",
        "    # Load the WikiMIA dataset\n",
        "    try:\n",
        "        # First, try to load the prepared dataset directly\n",
        "        ds_dev = load_from_disk(\"data/ds_dev\")\n",
        "        ds_test = load_from_disk(\"data/ds_test\")\n",
        "        print(\"Loaded prepared dataset from disk\")\n",
        "    except:\n",
        "        # If the prepared dataset is not found, process from scratch\n",
        "        print(\"Prepared dataset not found, processing WikiMIA data from scratch...\")\n",
        "\n",
        "        # Load the WikiMIA dataset\n",
        "        ds = load_dataset(\"swj0419/WikiMIA\")\n",
        "\n",
        "        # Select only the length32 data\n",
        "        length32_data = ds[\"WikiMIA_length32\"]\n",
        "\n",
        "        # Separate member (label=1) and non-member (label=0) samples, and rename the 'input' field to 'text'\n",
        "        member_samples = []\n",
        "        nonmember_samples = []\n",
        "\n",
        "        for item in length32_data:\n",
        "            # Create a new sample dictionary, changing the 'input' field to 'text'\n",
        "            new_item = {\n",
        "                'text': item['input'],\n",
        "                'label': item['label']\n",
        "            }\n",
        "            # Keep other existing fields\n",
        "            for key, value in item.items():\n",
        "                if key not in ['input', 'text', 'label']:\n",
        "                    new_item[key] = value\n",
        "\n",
        "            # Classify based on the label\n",
        "            if item['label'] == 1:\n",
        "                member_samples.append(new_item)\n",
        "            else:\n",
        "                nonmember_samples.append(new_item)\n",
        "\n",
        "        print(f\"Number of member samples: {len(member_samples)}\")\n",
        "        print(f\"Number of non-member samples: {len(nonmember_samples)}\")\n",
        "\n",
        "        # Adjust dataset sizes\n",
        "        # Use the number of available samples to determine the split\n",
        "        available_member_count = len(member_samples)\n",
        "        available_nonmember_count = len(nonmember_samples)\n",
        "\n",
        "        # Set the number of member samples in the dev and test sets\n",
        "        # Considering there are only 387 member samples, we can take 140 for dev and the remaining 247 for test\n",
        "        dev_member_count = 140\n",
        "        test_member_count = available_member_count - dev_member_count  # should be 247\n",
        "\n",
        "        # Set the same number of non-member samples to maintain balance\n",
        "        dev_nonmember_count = dev_member_count\n",
        "        test_nonmember_count = test_member_count\n",
        "\n",
        "        # Set a random seed for reproducibility\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Split the member samples\n",
        "        dev_members = member_samples[:dev_member_count]\n",
        "        test_members = member_samples[dev_member_count:]\n",
        "\n",
        "        # Split the non-member samples\n",
        "        dev_nonmembers = nonmember_samples[:dev_nonmember_count]\n",
        "        test_nonmembers = nonmember_samples[dev_nonmember_count:dev_nonmember_count+test_nonmember_count]\n",
        "\n",
        "        # Combine the dev set\n",
        "        ds_dev_list = dev_members + dev_nonmembers\n",
        "        np.random.shuffle(ds_dev_list)  # Shuffle the order\n",
        "\n",
        "        # Combine the test set\n",
        "        ds_test_list = test_members + test_nonmembers\n",
        "        np.random.shuffle(ds_test_list)  # Shuffle the order\n",
        "\n",
        "        # Verify dataset sizes and label balance\n",
        "        print(f\"Dev set size: {len(ds_dev_list)}\")\n",
        "        print(f\"Number of member samples in Dev set: {sum(1 for item in ds_dev_list if item['label'] == 1)}\")\n",
        "        print(f\"Number of non-member samples in Dev set: {sum(1 for item in ds_dev_list if item['label'] == 0)}\")\n",
        "\n",
        "        print(f\"Test set size: {len(ds_test_list)}\")\n",
        "        print(f\"Number of member samples in Test set: {sum(1 for item in ds_test_list if item['label'] == 1)}\")\n",
        "        print(f\"Number of non-member samples in Test set: {sum(1 for item in ds_test_list if item['label'] == 0)}\")\n",
        "\n",
        "        # Convert to Dataset format\n",
        "        ds_dev = Dataset.from_list(ds_dev_list)\n",
        "        ds_test = Dataset.from_list(ds_test_list)\n",
        "\n",
        "        # Create the data directory if it doesn't exist\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "        # Save to disk\n",
        "        ds_dev.save_to_disk(\"data/ds_dev\")\n",
        "        ds_test.save_to_disk(\"data/ds_test\")\n",
        "\n",
        "        print(\"Datasets have been saved to the data/ds_dev and data/ds_test directories\")\n",
        "\n",
        "    # Process for each threshold\n",
        "    for threshold in thresholds:\n",
        "        # Set the current threshold\n",
        "        ACTIVATION_THRESHOLD = threshold\n",
        "\n",
        "        # 1. Get member and non-member samples from the development set\n",
        "        dev_member = [item for item in ds_dev if item['label'] == 1]\n",
        "        dev_nonmember = [item for item in ds_dev if item['label'] == 0]\n",
        "\n",
        "        # 2. Split the development set evenly into training and validation sets\n",
        "        train_member = dev_member[:len(dev_member)//2]\n",
        "        train_nonmember = dev_nonmember[:len(dev_nonmember)//2]\n",
        "\n",
        "        val_member = dev_member[len(dev_member)//2:]\n",
        "        val_nonmember = dev_nonmember[len(dev_nonmember)//2:]\n",
        "\n",
        "        # 3. Collect activation data\n",
        "        train_member_acts = collect_activations(train_member)\n",
        "        train_nonmember_acts = collect_activations(train_nonmember)\n",
        "\n",
        "        val_member_acts = collect_activations(val_member)\n",
        "        val_nonmember_acts = collect_activations(val_nonmember)\n",
        "\n",
        "        # 4. Build reference activation patterns\n",
        "        reference_patterns = build_reference_patterns(\n",
        "            train_member_acts + train_nonmember_acts\n",
        "        )\n",
        "\n",
        "        # 5. Calculate layer discrimination scores\n",
        "        layer_scores = calculate_layer_discrimination_scores(reference_patterns)\n",
        "\n",
        "        # 6. Select the most discriminative layers\n",
        "        discriminative_layers = select_discriminative_layers(layer_scores, top_n=10)\n",
        "\n",
        "        # 7. Find the best threshold on the validation set using the relative ratio method\n",
        "        val_samples = val_member_acts + val_nonmember_acts\n",
        "        best_threshold, val_metrics = find_best_relative_ratio_threshold(\n",
        "            val_samples,\n",
        "            reference_patterns,\n",
        "            discriminative_layers\n",
        "        )\n",
        "\n",
        "        # 8. Calculate test set AUC\n",
        "        test_auc = calculate_auc(list(ds_test), reference_patterns, discriminative_layers)\n",
        "\n",
        "        # 9. Print only the dataset, threshold, and test set AUC value\n",
        "        print(f\"Dataset: WikiMIA, Threshold: {threshold:.1f}, Test Set AUC = {test_auc:.4f}\")\n",
        "\n",
        "        # 10. Clean up memory\n",
        "        del train_member_acts, train_nonmember_acts, val_member_acts, val_nonmember_acts\n",
        "        del reference_patterns, layer_scores, discriminative_layers, val_samples\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Release memory after processing the current dataset\n",
        "    del ds_dev, ds_test\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Set the activation thresholds to be evaluated\n",
        "        activation_thresholds = [1.0]\n",
        "\n",
        "        # Only process the WikiMIA dataset\n",
        "        process_wikimia(thresholds=activation_thresholds)\n",
        "\n",
        "        # After processing, remove the hooks\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {str(e)}\")\n",
        "        # Ensure hooks are removed\n",
        "        for hook in hooks:\n",
        "            try:\n",
        "                hook.remove()\n",
        "            except:\n",
        "                pass\n",
        "        raise e"
      ],
      "metadata": {
        "id": "AiomT02wLMk2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}